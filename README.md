Project n√†y ƒë∆∞·ª£c t·∫°o ra ch·ªß y·∫øu ƒë·ªÉ h·ªçc v·ªÅ LangChain v√† LangGraph.

# Fengshui Copilot ‚Äî Quick Start

> B·∫£n h∆∞·ªõng d·∫´n ch·∫°y nhanh cho d·ª± √°n Django RAG (LangChain + LangGraph) d√πng Supabase, HF Inference v√† OpenRouter/Ollama.

## 0) Y√™u c·∫ßu
- Python 3.12+, Git, venv  
- Supabase (Project + b·∫£ng `documents` v·ªõi c·ªôt `embedding` ki·ªÉu `vector(1024)`)  
- HF API Token (d√πng cho embeddings)  
- 1 LLM provider: **OpenRouter** *ho·∫∑c* **Ollama (llama3.1:8b)**

> N·∫øu ƒë·ªïi model embedding kh√°c **1024d**, h√£y s·ª≠a k√≠ch th∆∞·ªõc c·ªôt `vector(‚Ä¶)` v√† h√†m SQL t∆∞∆°ng ·ª©ng.

---

## 1) Clone & c√†i ƒë·∫∑t
* Clone repo:
```bash
git clone https://github.com/relieq/fengshui-copilot.git
```
* T·∫°o ti·∫øp m√¥i tr∆∞·ªùng ·∫£o.
* C√†i ƒë·∫∑t package:
```bash
pip install -U pip
pip install -r requirements.txt
```

---

## 2) C·∫•u h√¨nh `.env` (m·∫´u)
T·∫°o file `.env` ·ªü th∆∞ m·ª•c g·ªëc:

```env
# Provider & Model
LLM_PROVIDER=openrouter
LLM_MODEL=x-ai/grok-4-fast:free
#GRADE_MODEL=deepseek/deepseek-chat-v3.1:free
#ANSWER_MODEL=x-ai/grok-4-fast:free
#JUDGE_MODEL=google/gemini-2.0-flash-exp:free
#REWRITE_MODEL=tngtech/deepseek-r1t2-chimera:free
#MQR_MODEL=x-ai/grok-4-fast:free

# Ollama
RAG_EMBEDDING_MODEL=bge-m3

# OpenRouter
OPENROUTER_API_KEY=sk-or-xxxxxxxx
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1/
OPENROUTER_HTTP_REFERER=http://localhost:8000
OPENROUTER_APP_TITLE=fengshui-copilot-dev

# Embeddings (Hugging Face)
EMBED_PROVIDER=hf_endpoint
EMBEDDING_MODEL=BAAI/bge-m3
HUGGINGFACEHUB_API_TOKEN=hf_xxxxxxxxxxxxxxxx

# Supabase (server-side ONLY)
SUPABASE_URL=https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOi...
SUPABASE_TABLE=documents
SUPABASE_QUERY_NAME=match_documents
```

---

## 3) T·∫°o b·∫£ng & h√†m t√¨m ki·∫øm tr√™n Supabase
Trong SQL Editor c·ªßa Supabase, t·∫°o b·∫£ng v√† h√†m (ƒëi·ªÅu ch·ªânh n·∫øu b·∫°n ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥):

```sql
create extension if not exists vector;

create table if not exists documents (
    id bigserial primary key,
    uid text unique,
    content text,
    metadata jsonb, 
    embedding vector(1024),
);

create or replace function match_documents(
    filter jsonb default '{}'::jsonb,
    match_count int default 4,
    query_embedding vector(1024) default NULL
) returns table (
    id bigint,
    uid text,
    content text,
    metadata jsonb,
    embedding vector(1024),
    similarity double precision
) language sql stable as $$
    select
        d.id,
        d.uid,
        d.content,
        d.metadata,
        d.embedding,
        1 - (d.embedding <=> query_embedding) as similarity 
    from documents as d
    where d.metadata @> filter
    order by d.embedding <=> query_embedding
    limit match_count;
$$;

create index if not exists documents_embedding_idx 
    on documents using ivfflat (embedding vector_cosine_ops)
    with (lists = 100);
```

---

## 4) Chu·∫©n b·ªã d·ªØ li·ªáu
ƒê·∫∑t t√†i li·ªáu v√†o:  
`data/corpus/` (h·ªó tr·ª£ `.txt`, `.md`, `.pdf`).  
N·∫øu c√≥ PDF l·ªói m√£, file m·∫´u ƒë√£ c√≥ x·ª≠ l√Ω `decode_pdf_text()` cho m·ªôt s·ªë case.

---

## 5) Ingest (ƒë∆∞a d·ªØ li·ªáu v√†o Supabase)
```bash
python manage.py ingest_corpus
# ho·∫∑c l√†m m·ªõi s·∫°ch:
# python manage.py ingest_corpus --reset
```

---

## 6) H·ªèi ƒë√°p qua CLI
```bash
python manage.py qa_graph --q "M·ªánh Kim h·ª£p m√†u g√¨?" --k 6 --iters 2
```
- ƒê·ªì th·ªã LangGraph: `retrieve ‚Üí grade ‚Üí answer ‚Üí judge ‚Üí (rewrite?)`.  
- C√≥ `--deterministic` ƒë·ªÉ thread_id ·ªïn ƒë·ªãnh theo c√¢u h·ªèi.

---

## 7) Ch·∫°y server & API
```bash
python manage.py runserver
```

### Non-stream API
```
POST http://127.0.0.1:8000/api/ask
Content-Type: application/json

{"question": "M·ªánh Kim h·ª£p m√†u g√¨?", "k": 6, "iters": 2}
```
‚Üí tr·∫£ JSON: `answer`, `sources`, `verdict`, `thread_id`.

### Streaming (SSE)
M·ªü trang test: `http://127.0.0.1:8000/ask`, b·∫•m **H·ªèi (stream)** (c√≥ th·ªÉ l√†m t∆∞∆°ng t·ª± v·ªõi non-stream).
B·∫°n s·∫Ω th·∫•y c√°c event: `phase`, `source`, `grade`, `token`, `verdict`, `final`.

---

## 8) C·∫•u tr√∫c d·ª± √°n (r√∫t g·ªçn)
```
copilot/
  graph/
    rag_graph.py        # LangGraph nodes + build_graph
    runner.py           # run_graph() d√πng chung cho CLI/API
  llm/
    provider.py         # get_chat(role=‚Ä¶, model=‚Ä¶)
    embeddings.py
  rag/
    ingest.py           # ingest Supabase
    retriever.py        # MMR/MQ/similarity
  prompts/
    answer_prompt    # ---system / ---user blocks
    grader_prompt
    judge_prompt
    rewrite_prompt
  views/
    api.py              # /api/ask, /api/ask/stream (SSE)
    pages.py            # /ask demo page
  templates/
    ask.html
```

---

## 9) L·ªói th∆∞·ªùng g·∫∑p & c√°ch x·ª≠ l√Ω nhanh
- **HF 504** khi embed nhi·ªÅu: ƒë√£ chia batch; n·∫øu v·∫´n g·∫∑p ‚Üí gi·∫£m batch (vd 32) ho·∫∑c th·ª≠ l·∫°i.  
- **Dimension mismatch**: s·ª≠a `vector(N)` + re-ingest.  
- **SSE kh√¥ng ra token**: ki·ªÉm tra request/response headers;
- **LangGraph checkpointer l·ªói serialize**: d·ª± √°n d√πng `CustomSerdeProtocol(JsonPlusSerializer)` ƒë·ªÉ l·ªçc `callable` kh·ªèi state.

---

## 10) Tu·ª≥ ch·ªânh nhanh
- `.env`:
  - `RETRIEVER_MODE=mmr|similarity|mq`
  - `MODEL_ANSWER`, `MODEL_GRADER`, `MODEL_JUDGE`, `MODEL_REWRITER` (per-node model)
- Prompt: s·ª≠a trong `copilot/prompts`.

Ch√∫c b·∫°n build vui v·∫ª! üéã

# Fengshui Copilot ‚Äî H∆∞·ªõng d·∫´n chi ti·∫øt
**Features**
* Q&A phong th·ªßy c√≥ tr√≠ch d·∫´n ngu·ªìn (RAG)
* Tool: ƒë·ªïi l·ªãch, g·ª£i √Ω ng≈© h√†nh/m√†u...
* LangGraph: t·ª± ch·∫•m ƒëi·ªÉm, re-retrieve khi c·∫ßn.

Project n√†y s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán th√¥ng qua chu·ªói b√†i h·ªçc sau:
* B√†i 1 ‚Äî Chu·∫©n b·ªã m√¥i tr∆∞·ªùng + Django skeleton + Hello LLM
* B√†i 2 ‚Äî LangChain cƒÉn b·∫£n (Prompt ‚Üí Model ‚Üí Output Parser)
* B√†i 3 ‚Äî RAG v1: ingest t√†i li·ªáu phong th·ªßy (local)
* B√†i 4 ‚Äî ƒê√°nh gi√° nh·ªè
* B√†i 5 ‚Äî LangGraph: v√≤ng l·∫∑p t·ª±-ch·∫•m
* B√†i 6 ‚Äî Tool Use + Router
* B√†i 7 ‚Äî Streaming
* B√†i 8 ‚Äî ƒê√≥ng g√≥i (N·∫øu sau n√†y r·∫£nh)

_Ch√∫ √Ω:_ C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt ƒë∆∞·ª£c li·ªát k√™ trong requirements.txt

# B√†i 1: Chu·∫©n b·ªã m√¥i tr∆∞·ªùng + Django skeleton + Hello LLM
## M·ª•c ti√™u
* T·∫°o venv, c√†i base: Django, langchain, langgraph, chromadb, python-dotenv
* T·∫°o app copilot
* T·∫°o l·ªánh qu·∫£n tr·ªã hello_llm g·ªçi LLM (Ollama): in c√¢u ch√†o, log th·ªùi gian ph·∫£n h·ªìi.

## C√°c b∆∞·ªõc th·ª±c hi·ªán
1. C√†i Ollama, pull m√¥ h√¨nh v·ªÅ (c√°c b·∫°n c√≥ th·ªÉ l·ª±a ch·ªçn m√¥ h√¨nh kh√°c):
```bash
ollama pull llama3.1:8b
```
2. C√†i ƒë·∫∑t package, t·∫°o app copilot: python manage.py startapp copilot
3. N·∫°p .env tr∆∞·ªõc khi Django ch·∫°y
```python
def main():
    """Run administrative tasks."""
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "fengshui.settings")
    load_dotenv() # n·∫°p .env
    try:...
```

4. T·∫°o l·ªánh qu·∫£n tr·ªã hello_llm trong copilot/management/commands/hello_llm.py
**Django management command l√† g√¨?**
* L√† l·ªánh CLI t√πy bi·∫øn ch·∫°y qua `python manage.py <t√™n_l·ªánh>`
* D√πng khi b·∫°n mu·ªën vi·∫øt script c√≥ ng·ªØ c·∫£nh Django ƒë·∫ßy ƒë·ªß (ƒë√£ load settings, app, db...) - ti·ªán cho vi·ªác g·ªçi LLM,...
* <t√™n_l·ªánh> = t√™n file, Django s·∫Ω t·ª± d√≤ c√°c file trong your_app/management/commands/*.py. B√™n trong ƒë√≥ c√≥ 1 l·ªõp t√™n 
Command k·∫ø th·ª´ BaseCommand, Django s·∫Ω g·ªçi l·ªõp n√†y khi ch·∫°y l·ªánh.
```python
class Command(BaseCommand):
    help = "G·ªçi LLM ch√†o h·ªèi ng∆∞·ªùi d√πng, c·∫•u h√¨nh qua .env"

    def handle(self, *args, **kwargs):
        model = os.getenv("LLM_MODEL")
        llm = ChatOllama(model=model, temperature=0)

        prompt = "B·∫°n l√† tr·ª£ l√Ω v·ªÅ phong th·ªßy, h√£y ch√†o t√¥i b·∫±ng 1-2 c√¢u ti·∫øng Vi·ªát."

        t0 = time.time()
        res = llm.invoke(prompt)

        self.stdout.write(f"[{model}] {res.content} \nTook {time.time() - t0:.2f}s")
```
Ch·∫°y l·ªánh `python manage.py hello_llm`, ta s·∫Ω th·∫•y k·∫øt qu·∫£ theo m·∫´u sau:
```
[llama3.1:8b] Ch√†o b·∫°n! T√¥i r·∫•t vui ƒë∆∞·ª£c g·∫∑p v√† h·ªó tr·ª£ b·∫°n trong lƒ©nh v·ª±c phong th·ªßy. B·∫°n c·∫ßn t∆∞ v·∫•n g√¨ h√¥m nay? 
Took 19.87s
```

# B√†i 2: LangChain cƒÉn b·∫£n (Prompt ‚Üí Model ‚Üí Output Parser)
## LangChain l√† g√¨?
* LangChain l√† b·ªô "lego" gi√∫p b·∫°n l·∫Øp gh√©p c√°c b∆∞·ªõc l√†m vi·ªác v·ªõi LLM: so·∫°n prompt ‚Üí g·ªçi model ‚Üí √©p ƒë·ªãnh d·∫°ng 
‚Üí (t√πy ch·ªçn) t√¨m t√†i li·ªáu (RAG) ‚Üí (t√πy ch·ªçn) g·ªçi tool ‚Üí tr·∫£ k·∫øt qu·∫£.
* B·∫°n l·∫Øp c√°c b∆∞·ªõc tr√™n b·∫±ng th·ª© g·ªçi l√† "·ªëng n·ªëi" LCEL (|). V√≠ d·ª•: `PromptTemplate | ChatModel | OutputParser`
* M·ªôt s·ªë m·∫£nh "lego" ph·ªï bi·∫øn:
  * PromptTemplate: khu√¥n l·ªùi nh·∫Øc c√≥ bi·∫øn ({question})
  * ChatModel: nh∆∞ ch√∫ng ta d√πng l√† Ollama
  * OutputParser: √©p ƒë·ªãnh d·∫°ng tr·∫£ v·ªÅ c·ªßa model (JSON, text,...)
  * DocumentLoader + TextSplitter: ƒë·ªçc t√†i li·ªáu v√† t√°ch th√†nh c√°c ƒëo·∫°n nh·ªè (cho RAG)
  * Embedding + VectorStore: chuy·ªÉn ƒëo·∫°n vƒÉn th√†nh vector r·ªìi l∆∞u ƒë·ªÉ t√¨m theo ng·ªØ nghƒ©a
  * Retriever: l·∫•y top-k ƒëo·∫°n li√™n quan cho c√¢u h·ªèi
  * Memory: l∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i (t√πy ch·ªçn)

  ...
* LangChain gi√∫p ch√∫ng ta t·∫≠p trung v√†o logic c·ªßa AI m√† kh√¥ng ph·∫£i vi·∫øt tay m·ªçi k·∫øt n·ªëi (I/O, format,...) t·ª´ ƒë·∫ßu.

**M·ª•c ti√™u b√†i 2**: t·∫°o l·ªánh structured_qa - m·∫´u "Prompt ‚Üí Model ‚Üí Output Parser" (v·ªÅ sau c√≥ th·ªÉ d√πng ƒë·ªÉ debug)
**C√°c b∆∞·ªõc th·ª±c hi·ªán**
1. T·∫°o l·ªánh qu·∫£n tr·ªã structured_qa trong copilot/management/commands/
2. Khai b√°o pydantic schema ƒë·ªÉ √©p ƒë·ªãnh d·∫°ng tr·∫£ v·ªÅ:
* Output Parser l√† l·ªõp h·∫≠u x·ª≠ l√Ω trong LangChain. D√π ƒë√£ nh·∫Øc LLM b·∫±ng format_instructions, parser v·∫´n c·∫ßn ƒë·ªÉ chuy·ªÉn 
chu·ªói ƒë·∫ßu ra th√†nh d·ªØ li·ªáu c√≥ c·∫•u tr√∫c (Pydantic model), ki·ªÉm tra & validate (ki·ªÉu, r√†ng bu·ªôc, thi·∫øu tr∆∞·ªùng), v√† n√©m 
l·ªói s·ªõm khi sai. Nh·ªù ƒë√≥, pipeline ·ªïn ƒë·ªãnh v√† code ph√≠a sau d√πng d·ªØ li·ªáu nh∆∞ object Python thay v√¨ vƒÉn b·∫£n t·ª± do.
```python
class FengshuiAnswer(BaseModel):
    # D·∫•u ... (g·ªçi l√† ellipsis) ·ªü ƒë√¢y l√† m·ªôt gi√° tr·ªã ƒë·∫∑c bi·ªát t·ª´ module builtins c·ªßa Python, ƒë·∫°i di·ªán cho vi·ªác
    # field n√†y b·∫Øt bu·ªôc ph·∫£i c√≥ gi√° tr·ªã (required) v√† kh√¥ng c√≥ gi√° tr·ªã m·∫∑c ƒë·ªãnh (no default value).
    # N·∫øu mu·ªën c√≥ default, b·∫°n thay ... b·∫±ng gi√° tr·ªã c·ª• th·ªÉ, v√≠ d·ª• Field("M·∫∑c ƒë·ªãnh").
    answer: str = Field(..., description="C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi√™t, ~80 t·ª´")
    citations: List[str] = Field(
        default_factory=list,
        description="C√°c kh√°i ni·ªám/ngu·ªìn li√™n quan (v√≠ d·ª•: ng≈© h√†nh, b√°t qu√°i,...)"
    )
    confidence: float = Field(
        0.6, ge=0.0, le=1.0,
        description="ƒê·ªô tin c·∫≠y t·ª´ 0.0 ƒë·∫øn 1.0"
    )
```
3. T·∫°o prompt template:
* T·∫°o format instruction d·ª±a tr√™n parser tr∆∞·ªõc ƒë√≥ (ch√∫ √Ω ƒë√¢y ch·ªâ l√† l·ªùi nh·∫Øc cho model th√¥i, v·ªÅ sau parser trong chain
v·∫´n ph·∫£i ki·ªÉm tra v√† √©p v·ªÅ ch√≠nh x√°c ƒë·ªãnh d·∫°ng ch√∫ng ta y√™u c·∫ßu):
```python
class Command(BaseCommand):
    help = "H·ªèi ƒë√°p phong th·ªßy (LangChain + Ollama) c√≥ √©p JSON"

    def add_arguments(self, parser):
        parser.add_argument("--q", dest="question", required=False,
                            default="M·ªánh Kim h·ª£p m√†u g√¨?", help="C√¢u h·ªèi ti·∫øng Vi·ªát")

    def handle(self, *args, **opts):
        model = os.getenv("LLM_MODEL")
        llm = ChatOllama(model=model, temperature=0)

        # T·∫°o parser + format instructions
        parser = PydanticOutputParser(pydantic_object=FengshuiAnswer)
        format_instructions = parser.get_format_instructions()
```
* T·∫°o prompt template:
```python
prompt = ChatPromptTemplate.from_messages([
    ("system",
     "B·∫°n l√† chuy√™n gia phong th·ªßy, h√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, kh√¥ng b·ªãa"
     " CH·ªà TR·∫¢ V·ªÄ D·ªÆ LI·ªÜU THEO SCHEMA JSON ƒë∆∞·ª£c cung c·∫•p"),
    ("human",
     "C√¢u h·ªèi: {question}\n\n"
     "H√£y tu√¢n th·ªß nghi√™m ng·∫∑t h∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng sau:\n {format_instructions}")
])
```
4. T·∫°o chain v√† ch·∫°y:
```python
# T·∫°o chain:
chain = prompt | llm | parser

t0 = time.time()
try:
    # ƒê√¢y ch√≠nh l√† t√°c d·ª•ng c·ªßa parser
    res: FengshuiAnswer = chain.invoke({
        "question": opts["question"],
        "format_instructions": format_instructions
    })
# V√¨ th·ª´a, kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng n√™n kh√¥ng th·ªÉ parse ƒë∆∞·ª£c
except Exception as e:
    self.stderr.write(self.style.WARNING(f"Parse l·ªói, th·ª≠ fallback: {e}"))
    raw = (prompt | llm).invoke({
        "question": opts["question"],
        "format_instructions": format_instructions
    }).content

    match = re.search(r"\{.*\}", raw, re.DOTALL) # T√¨m ƒëo·∫°n JSON d√†i nh·∫•t trong raw nh·∫±m c·∫Øt ra kh·ªèi vƒÉn b·∫£n th·ª´a
    if not match:
        raise CommandError("Kh√¥ng t√¨m th·∫•y JSON trong c√¢u tr·∫£ l·ªùi")
    res = parser.parse(match.group(0))

dt = time.time() - t0

self.stdout.write(json.dumps(res.dict(), ensure_ascii=False, indent=2))
self.stdout.write(self.style.SUCCESS(f"[{model}] took {dt:.2f}s"))
```
Test th·ª≠ v·ªõi l·ªánh:
```bash
python manage.py structured_qa --q "Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p m·ªánh n√†o?"
````
Ta s·∫Ω nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£ t∆∞∆°ng t·ª± nh∆∞ sau:
```json
{
  "answer": "Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p v·ªõi ng∆∞·ªùi m·ªánh M·ªôc v√† Th·ªßy",
  "citations": [
    "Ng≈© h√†nh",
    "B√°t qu√°i"
  ],
  "confidence": 0.8
}
[llama3.1:8b] took 95.44s
```

**T·ªïng k·∫øt b√†i 2:**
* Ch√∫ng ta ƒë√£ t·∫°o ƒë∆∞·ª£c l·ªánh h·ªèi ƒë√°p phong th·ªßy c√≥ √©p ƒë·ªãnh d·∫°ng tr·∫£
* Ch√∫ng ta ƒë√£ l√†m quen v·ªõi c√°c m·∫£nh "lego" c∆° b·∫£n c·ªßa LangChain: PromptTemplate, ChatModel, OutputParser
* Ch√∫ng ta ƒë√£ th·∫•y ƒë∆∞·ª£c s·ª©c m·∫°nh c·ªßa OutputParser trong vi·ªác ki·ªÉm so√°t ƒë·ªãnh d·∫°ng tr·∫£ v·ªÅ, gi√∫p pipeline ·ªïn ƒë·ªãnh h∆°n.
V·ªÅ sau, ch√∫ng ta s·∫Ω ti·∫øp t·ª•c x√¢y d·ª±ng d·ª±a tr√™n pipeline n√†y ƒë·ªÉ th√™m RAG, tool, memory,...

# B√†i 3: RAG v1 (Ingest ‚Üí Split ‚Üí Embed ‚Üí Retrieve)
## RAG l√† g√¨?
* RAG (Retrieval-Augmented Generation) l√† k·ªπ thu·∫≠t k·∫øt h·ª£p LLM v·ªõi h·ªá th·ªëng t√¨m ki·∫øm t√†i li·ªáu ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c v√†
t√≠nh c·∫≠p nh·∫≠t c·ªßa c√¢u tr·∫£ l·ªùi, gi√∫p c√¢u tr·∫£ l·ªùi b√°m v√†o d·ªØ ki·ªán t√†i li·ªáu c·ªßa ch√∫ng ta, gi·∫£m b·ªãa ƒë·∫∑t.
* √ù t∆∞·ªüng ch√≠nh: tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi, LLM s·∫Ω t√¨m ki·∫øm c√°c t√†i li·ªáu li√™n quan trong kho d·ªØ li·ªáu (vector store) v√† s·ª≠ d·ª•ng
ch√∫ng l√†m ngu·ªìn tham kh·∫£o ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi.
* Quy tr√¨nh RAG g·ªìm c√°c b∆∞·ªõc:
  1. Ingest: thu th·∫≠p v√† n·∫°p t√†i li·ªáu v√†o h·ªá th·ªëng.
  2. Split: t√°ch t√†i li·ªáu th√†nh c√°c ƒëo·∫°n nh·ªè ƒë·ªÉ d·ªÖ qu·∫£n l√Ω v√† t√¨m ki·∫øm.
  3. Embed: chuy·ªÉn c√°c ƒëo·∫°n vƒÉn b·∫£n th√†nh vector (embedding) ƒë·ªÉ l∆∞u tr·ªØ v√† t√¨m ki·∫øm theo ng·ªØ nghƒ©a.
  4. Store: l∆∞u tr·ªØ c√°c vector trong c∆° s·ªü d·ªØ li·ªáu vector (vector store, ·ªü ƒë√¢y ch√∫ng ta d√πng Chroma).
  5. Retrieve: t√¨m ki·∫øm v√† l·∫•y c√°c ƒëo·∫°n vƒÉn b·∫£n li√™n quan t·ª´ vector store d·ª±a tr√™n c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
  6. Generate: K·∫øt h·ª£p th√¥ng tin truy xu·∫•t ƒë∆∞·ª£c v√†o prompt g·ª≠i ƒë·∫øn LLM (tƒÉng c∆∞·ªùng). LLM t·∫°o ra c√¢u tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ 
  li·ªáu tƒÉng c∆∞·ªùng, ƒë·∫£m b·∫£o t√≠nh c·∫≠p nh·∫≠t v√† ch√≠nh x√°c.

## C√°c b∆∞·ªõc th·ª±c hi·ªán
1. C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt: langchain-chroma, chromadb, langchain-textsplitters

    Pull embedding model cho Ollama: `ollama pull bge-m3` ho·∫∑c nomic-embed-text
2. Chu·∫©n b·ªã t√†i li·ªáu phong th·ªßy:
* T·∫°o th∆∞ m·ª•c fengshui-copilot/data/corpus ch∆∞a c√°c file t√†i li·ªáu v·ªÅ phong th·ªßy (md, txt, pdf,...)
* T√¥i c√≥ cho s·∫µn m·ªôt s·ªë t√†i li·ªáu, b·∫°n ƒë·ªçc c√≥ th·ªÉ b·ªï sung th√™m.

3. T·∫°o file c√†i ƒë·∫∑t c√°c tham s·ªë h·ªá th·ªëng:
* T·∫°o file copilot/rag/settings.py:
```python
ROOT_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = ROOT_DIR / "data"

CORPUS_DIR = Path(os.getenv("RAG_CORPUS_DIR", DATA_DIR / "corpus"))
CHROMA_DIR = Path(os.getenv("RAG_CHROMA_DIR", DATA_DIR / "chroma"))
COLLECTION_NAME = os.getenv("RAG_COLLECTION_NAME", "fengshui")

# Tham s·ªë split & retrieve
CHUNK_SIZE = int(os.getenv("RAG_CHUNK_SIZE", 800))
CHUNK_OVERLAP = int(os.getenv("RAG_CHUNK_OVERLAP", 120))
TOP_K = int(os.getenv("RAG_TOP_K", 4))

# Model
EMBEDDING_MODEL = os.getenv("RAG_EMBEDDING_MODEL") # T√¥i d√πng bge-m3
LLM_MODEL = os.getenv("LLM_MODEL")

# ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
def ensure_dirs():
    CORPUS_DIR.mkdir(parents=True, exist_ok=True)
    CHROMA_DIR.mkdir(parents=True, exist_ok=True)
```
* CHUNK_SIZE: k√≠ch th∆∞·ªõc t·ªëi ƒëa c·ªßa m·ªói chunk (ƒëo·∫°n vƒÉn b·∫£n nh·ªè) sau khi split
* CHUNK_OVERLAP: ƒë·ªô ch·ªìng l·∫Øp gi·ªØa c√°c chunk (gi√∫p gi·ªØ ng·ªØ c·∫£nh) - t·ª©c l√† m·ªói c·∫∑p chunk li·ªÅn k·ªÅ s·∫Ω c√≥ m·ªôt ph·∫ßn n·ªôi dung tr√πng nhau.
* TOP_K: s·ªë ƒëo·∫°n vƒÉn b·∫£n li√™n quan s·∫Ω l·∫•y ra ƒë·ªÉ tƒÉng c∆∞·ªùng cho LLM

4. T·∫°o file n·∫°p (ingest) t√†i li·ªáu:
* T·∫°o file copilot/rag/ingest.py
* Tr∆∞·ªõc ti√™n, ch√∫ng ta vi·∫øt 2 h√†m load file:
```python
def _load_text_file(path: Path) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def _load_pdf_file(path:Path) -> str:
    text = []
    try:
        reader = PdfReader(str(path))
        for page in reader.pages:
            text.append(page.extract_text() or "")
    except Exception:
        return ""

    return "\n".join(text)
```
* Vi·∫øt h√†m load_corpus ƒë·ªÉ duy·ªát th∆∞ m·ª•c, ƒë·ªçc c√°c file c√≥ ƒëu√¥i ph√π h·ª£p r·ªìi tr·∫£ v·ªÅ danh s√°ch LangChain Document:
```python
def load_corpus(corpus_dir: Path) -> List[Document]:
    EXTS = (".txt", ".md", ".markdown", ".mdx", ".pdf")
    docs = []

    for p in corpus_dir.rglob("*"):
        if p.is_file() and p.suffix.lower() in EXTS and not p.name.startswith("."):
            try:
                if p.suffix.lower() == ".pdf":
                    content = _load_pdf_file(p)
                else:
                    content = _load_text_file(p)
            except Exception as e:
                print(f"[INGEST] Skip {p} ({e})")
                continue

            content = (content or "").strip() # ph·∫£i (content or "") v√¨ h√†m _load_text_file c√≥ th·ªÉ tr·∫£ v·ªÅ None
            if not content:
                continue
            docs.append(Document(
                page_content=content,
                metadata={"source": str(p.relative_to(corpus_dir))} # ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªëi v·ªõi corpus_dir
                # th√™m metadata ƒë·ªÉ sau n√†y hi·ªán tr√≠ch d·∫´n trong c√¢u tr·∫£ l·ªùi
            ))

    return docs
```
* Vi·∫øt h√†m chunk_documents ƒë·ªÉ t√°ch c√°c Document l·ªõn th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n c√≥ start_index (v·ªã tr√≠ b·∫Øt ƒë·∫ßu trong vƒÉn b·∫£n g·ªëc) ƒë·ªÉ _make_id sau n√†y t·∫°o id ·ªïn ƒë·ªãnh:
```python
def chunk_documents(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP,
        add_start_index=True
    )
    return splitter.split_documents(docs)

def _make_id(doc: Document) -> str:
    src = doc.metadata.get("source", "")
    start = doc.metadata.get("start_index", None)
    return f"{src}::{start}"
```
* Vi·∫øt h√†m build_or_update_chroma ƒë·ªÉ t·∫°o ho·∫∑c c·∫≠p nh·∫≠t Chroma vector store:
```python
def build_or_update_chroma(chunks: List[Document], reset: bool = False) -> int:
    ensure_dirs()

    if reset and CHROMA_DIR.exists():
        shutil.rmtree(CHROMA_DIR)

    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

    # Chroma vector store
    vs = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=str(CHROMA_DIR)
    )

    # T·∫°o id ·ªïn ƒë·ªãnh ƒë·ªÉ tr√°nh tr√πng l·∫∑p n·∫øu ingest nhi·ªÅu l·∫ßn
    seen = set()
    docs_unique, ids_unique = [], []
    for doc in chunks:
        _id = _make_id(doc)
        if _id in seen:
            continue
        seen.add(_id)
        docs_unique.append(doc)
        ids_unique.append(_id)

    vs.add_documents(docs_unique, ids=ids_unique)

    return len(ids_unique)
```
* Vi·∫øt h√†m ingest_corpus t·ªïng h·ª£p c√°c b∆∞·ªõc tr√™n:
```python
def ingest_corpus(reset: bool = False) -> dict:
    ensure_dirs()
    docs = load_corpus(CORPUS_DIR)
    chunks = chunk_documents(docs) if docs else []
    n = build_or_update_chroma(chunks, reset=reset) if chunks else 0
    return {
        "files": len(docs),
        "chunks": len(chunks),
        "added": n,
        "corpus_dir": str(CORPUS_DIR),
        "chroma_dir": str(CHROMA_DIR),
        "collection": COLLECTION_NAME,
    }
```
5. T·∫°o file truy xu·∫•t (retrieve) t√†i li·ªáu:
* T·∫°o file copilot/rag/retriever.py
```python
def get_retriever(top_k: int | None = None):
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

    vs = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=str(CHROMA_DIR)
    )
    return vs.as_retriever(search_kwargs={"k": top_k or TOP_K})
```
6. T·∫°o 2 l·ªánh qu·∫£n tr·ªã ƒë·ªÉ ingest v√† h·ªèi th·ª≠ RAG:
* T·∫°o l·ªánh ingest_corpus trong copilot/management/commands/ingest_corpus.py
```python
def Command(BaseCommand):
    help = "Ingest t√†i li·ªáu phong thu·ª∑ v√†o Chroma (embed->split)"

    def add_arguments(self, parser):
        parser.add_argument(
            "--reset", action="store_true",
            help="X√≥a index c≈© tr∆∞·ªõc khi ingest"
        )

    def handle(self, *args, **opts):
        t0 = time.time()
        stats = ingest_corpus(reset=opts["reset"])
        dt = time.time() - t0

        self.stdout.write(self.style.SUCCESS(
            f"INGEST DONE in {dt:.2f}s | files={stats['files']} "
            f"chunks={stats['chunks']} added={stats['added']}\n"
            f"corpus={stats['corpus_dir']} | chroma={stats['chroma_dir']} "
            f"| collection={stats['collection']}"
        ))
```
* T·∫°o l·ªánh rag_ask trong copilot/management/commands/rag_ask.py
```python
ANSWER_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "B·∫°n l√† tr·ª£ l√Ω phong thu·ª∑. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p."
     " N·∫øu ng·ªØ c·∫£nh kh√¥ng ƒë·ªß, h√£y n√≥i 'T√¥i kh√¥ng ch·∫Øc t·ª´ t√†i li·ªáu hi·ªán c√≥.'"),
    ("human",
     "C√¢u h·ªèi: {question}\n\n"
     "Ng·ªØ c·∫£nh (c√≥ th·ªÉ r√∫t g·ªçn):\n{context}\n\n"
     "Y√™u c·∫ßu:\n- Tr·∫£ l·ªùi 2‚Äì4 c√¢u ti·∫øng Vi·ªát, b√°m s√°t ng·ªØ c·∫£nh.\n"
     "- Li·ªát k√™ ngu·ªìn (t√™n file) ƒë√£ d√πng ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi.")
])

class Command(BaseCommand):
    help = "H·ªèi‚Äìƒë√°p v·ªõi RAG (retriever + LLM), in c√¢u tr·∫£ l·ªùi k√®m ngu·ªìn."

    def add_arguments(self, parser):
        parser.add_argument("--q", dest="question", required=True,
                            help="C√¢u h·ªèi phong th·ªßy (ti·∫øng Vi·ªát)")
        parser.add_argument("--k", dest="top_k", type=int, default=TOP_K,
                            help="S·ªë ƒëo·∫°n tr√≠ch d·∫´n l·∫•y v·ªÅ (top k)")
        parser.add_argument("--model", default=LLM_MODEL,
                            help="T√™n model Ollama")
        parser.add_argument("--temp", type=float, default=0.0)

    def handle(self, *args, **opts):
        q = opts["question"]
        k = opts["top_k"]
        model = opts["model"]
        temp = opts["temp"]

        # L·∫•y ng·ªØ c·∫£nh
        retriever = get_retriever(k)
        docs = retriever.invoke(q)

        # Gh√©p ƒëo·∫°n tr√≠ch t√†i li·ªáu + ngu·ªìn ƒë·ªÉ t·∫°o ng·ªØ c·∫£nh
        ctx_lines = []
        used_files = set()

        for i, d in enumerate(docs):
            snippet = d.page_content.strip().replace("\n", " ")

            # if len(snippet) > 500:
            #     snippet = snippet[:500] + "..."

            src = d.metadata.get("source", "")
            used_files.add(src)

            ctx_lines.append(f"[{i+1}] {snippet} (SOURCE: {src})")

        context = "\n\n".join(ctx_lines) if ctx_lines else "(Kh√¥ng c√≥ ng·ªØ c·∫£nh)"

        llm = ChatOllama(model=model, temperature=temp)
        chain = ANSWER_PROMPT | llm

        t0 = time.time()
        res = chain.invoke({
            "question": q,
            "context": context
        })
        dt = time.time() - t0

        answer = res.content.strip()
        sources = ", ".join(sorted(used_files)) if used_files else "Kh√¥ng c√≥"
        self.stdout.write(f"[{model}] {answer}\n\n"
                          f"Sources: {sources}\n"
                          f"Took {dt:.2f}s")
```
* Test th·ª≠:
* Ch·∫°y l·ªánh ingest_corpus: `python manage.py ingest_corpus` ho·∫∑c `python manage.py ingest_corpus --reset` ƒë·ªÉ x√≥a index c≈©.
* Ch·∫°y l·ªánh rag_ask:

M·ªçi ng∆∞·ªùi c√≥ th·ªÉ x√≥a index trong \chroma xem tr∆∞·ªõc k·∫øt qu·∫£ nh∆∞ n√†o, sau ƒë√≥ h·∫µng ingest l·∫°i d·ªØ li·ªáu ƒë·ªÉ th·∫•y hi·ªáu qu·∫£.
```bash
python manage.py rag_ask --q "M·ªánh Kim h·ª£p m√†u g√¨?"
python manage.py rag_ask --q "Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p m·ªánh n√†o?" --k 6
```
Output:
```
[llama3.1:8b] M·ªánh Kim h·ª£p v·ªõi m√†u s√°ng v√† nh·∫π nh√†ng nh∆∞ m√†u tr·∫Øng, v√†ng, √°nh kim. Nh·ªØng m√†u n√†y gi√∫p c√¢n b·∫±ng v√† h·ªó tr·ª£ cho ng∆∞·ªùi m·ªánh Kim.

Ngu·ªìn:
1. phong_thuy_toan_tap.pdf
2. phong_thuy_thuc_hanh_trong_xay_dung_va_kien_truc_nha_o.pdf

Sources: phong_thuy_thuc_hanh_trong_xay_dung_va_kien_truc_nha_o.pdf, phong_thuy_toan_tap.pdf
Took 260.47s
```
```
[llama3.1:8b] Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p v·ªõi ng∆∞·ªùi m·ªánh M·ªôc v√† Th·ªßy.

Ngu·ªìn: phong_thuy_toan_tap.pdf, phong_thuy_thuc_hanh_trong_xay_dung_va_kien_truc_nha_o.pdf

Sources: phong_thuy_thuc_hanh_trong_xay_dung_va_kien_truc_nha_o.pdf, phong_thuy_toan_tap.pdf
Took 299.03s
```

# B√†i 4: ƒê√°nh gi√° RAG
* M·ª•c ti√™u: c√≥ c√°c con s·ªë ƒë·ªÉ ch·ª©ng minh h·ªá RAG ho·∫°t ƒë·ªông.
* Evaluation set: 1 file jsonl, m·ªói d√≤ng g·ªìm:
  * q: c√¢u h·ªèi
  * sources: danh s√°ch t√™n file trong corpus ƒë∆∞·ª£c coi l√† ngu·ªìn ƒë√∫ng (file-level)
  * ref: c√¢u tr·∫£ l·ªùi tham chi·∫øu ng·∫Øn
* Trong b√†i n√†y, ch√∫ng ta s·∫Ω tri·ªÉn khai 3 c√°ch ƒë√°nh gi√° (2 ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng truy v·∫•n t√†i li·ªáu, 1 ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng 
c√¢u tr·∫£ l·ªùi):
  * Recall@k (file-level): % c√¢u h·ªèi m√† trong s·ªë tok-k chunk l·∫•y v·ªÅ c√≥ √≠t nh·∫•t 1 chunk t·ª´ ngu·ªìn ƒë√∫ng.
  * MRR@k (Mean Reciprocal Rank (x·∫øp h·∫°ng ngh·ªãch ƒë·∫£o trung b√¨nh)): trung b√¨nh c·ªßa 1/rank v·ªõi rank l√† v·ªã tr√≠ chunk kh·ªõp 
  ƒë√∫ng ƒë·∫ßu ti√™n v·ªõi sources trong top-k (kh√¥ng kh·ªõp ‚Üí 0).
  * Answer quality: ch·∫•m th√¥ b·∫±ng "lexical" F1 (F1 score b·∫£n t·ª´ ng·ªØ, so tr√πng t·ª´ gi·ªØa answer v√† ref). C√≥ th·ªÉ k√®m model 
  cho ƒëi·ªÉm 0..1 d·ª±a v√†o ƒë√∫ng/sai c·ªßa n·ªôi dung.

_L∆∞u √Ω:_ ·ªû ƒë√¢y ch√∫ng ta ch·ªâ m·ªõi tri·ªÉn khai ngang file-level, v·ªÅ sau c√≥ th·ªÉ n√¢ng c·∫•p l√™n chunk-level (t√≠nh
tr√πng t·ª´ trong chunk l·∫•y v·ªÅ v·ªõi chunk ƒë√∫ng).

## C√°c b∆∞·ªõc th·ª±c hi·ªán
### 1. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë√°nh gi√°
* L·∫ßn n√†y t√¥i c√≥ b·ªï sung th√™m 4 file t√†i li·ªáu n·ªØa v√†o m·ª•c data (ban ƒë·∫ßu 2). ·ªû b∆∞·ªõc n√†y, ch√∫ng ta c√≥ th·ªÉ nh·ªù c√°c AI agent 
t·∫°o gi√∫p ch√∫ng ta file d·ªØ li·ªáu ƒë√°nh gi√° qa.jsonl, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ tham kh·∫£o prompt ƒë∆°n gi·∫£n sau:
```
Hi·ªán t·∫°i t√¥i ƒëang mu·ªën t·∫°o m·ªôt t·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° cho m√¥ h√¨nh RAG trong project c·ªßa t√¥i v·ªõi t·∫≠p eval set l√† 1 file .jsonl m·ªói d√≤ng g·ªìm: 
- q: c√¢u h·ªèi 
- sources: danh s√°ch t√™n file trong corpus ƒë∆∞·ª£c coi l√† ngu·ªìn ƒë√∫ng (file-level) 
- ref (t√πy ch·ªçn): c√¢u tr·∫£ l·ªùi tham chi·∫øu ng·∫Øn 

M·∫´u: 
{"q": "M·ªánh Kim h·ª£p m√†u g√¨?", "sources": ["ngu_hanh_co_ban.md"], "ref": "M·ªánh Kim h·ª£p tr·∫Øng, x√°m, √°nh kim; t∆∞∆°ng sinh Th·ªï nh∆∞ v√†ng, n√¢u."} 
{"q": "Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p m·ªánh n√†o?", "sources": ["huong_nha_tom_tat.txt"], "ref": "ƒê√¥ng Nam thu·ªôc M·ªôc, th∆∞·ªùng h·ª£p m·ªánh M·ªôc v√† m·ªánh Ho·∫£ (t∆∞∆°ng sinh)."} 
{"q": "Ng≈© h√†nh g·ªìm nh·ªØng y·∫øu t·ªë n√†o?", "sources": ["ngu_hanh_co_ban.md"], "ref": "Kim, M·ªôc, Thu·ª∑, Ho·∫£, Th·ªï."} 

V·ªÅ ph·∫ßn source, t√¥i c√≥ g·ª≠i cho b·∫°n c√°c ngu·ªìn nh∆∞ tr√™n, b·∫°n h√£y load v√† ƒë·ªçc kƒ© c√°c file r·ªìi t·∫°o gi√∫p t√¥i file jsonl ph√≠a tr√™n. 
T√†i li·ªáu t√¨m ƒë∆∞·ª£c kh√° h·∫°n ch·∫ø, n·∫øu c√≥ th·ªÉ b·∫°n h√£y t·ª± t√¨m ki·∫øm, t·∫£i v·ªÅ, ph√¢n t√≠ch t√†i li·ªáu r·ªìi vi·∫øt th√™m v√†o file jsonl gi√∫p t√¥i. 

Ch√∫ √Ω ph·∫£i l√†m cho th·∫≠t ch√≠nh x√°c, ph·∫£i c√≥ √≠t nh·∫•t 100 c√¢u, ƒë·ªÉ t√¥i c√≥ th·ªÉ ƒë√°nh gi√° k·∫øt qu·∫£ m√¥ h√¨nh c·ªßa b·∫£n th√¢n m·ªôt c√°ch c√≥ hi·ªáu qu·∫£.
```
### 2. T·∫°o 2 l·ªánh ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng retrieval t√†i li·ªáu: Recall@k v√† MRR@k
* T·∫°o file copilot/rag/eval_retrieval.py
* T·∫°o h√†m ƒë·ªçc file jsonl:
```python
def read_jsonl(path: Path):
    if not path.exists():
        raise FileNotFoundError(path)
    with open(path, 'r', encoding='utf') as f:
        for line in f:
            line = line.strip()
            if line:
                yield json.loads(line)
```
* T·∫°o l·ªánh:
```python
class Command(BaseCommand):
    help = "ƒê√°nh gi√° retrieval ·ªü file-level v·ªõi 2 ph∆∞∆°ng ph√°p: Recall@k v√† MRR@k"

    def add_arguments(self, parser):
        parser.add_argument("--file", default=str(DATA_DIR / "eval" / "qa.jsonl"))
        parser.add_argument("--k", type=int, default=TOP_K)

    def handle(self, *args, **opts):
        eval_path = Path(opts["file"])
        k = opts["k"]

        data = list(read_jsonl(eval_path))
        if not data:
            raise CommandError(f"Eval set r·ªóng: {eval_path}")

        retriever = get_retriever(k)

        recalls, mrrs = [], []
        t0 = time.time()

        for i, item in enumerate(data):
            q = item["q"]
            gold = item.get("sources", "")
            docs = retriever.invoke(q)

            ranked_src = [d.metadata.get("source", "") for d in docs]
            hit = any(r in gold for r in ranked_src)
            recalls.append(hit)

            rr = 0.0
            for rank, src in enumerate(ranked_src):
                if src in gold:
                    rr = 1.0 / (rank + 1)
                    break
            mrrs.append(rr)

            self.stdout.write(
                f"[i] Q: {q}\n"
                f"gold: {gold}\n"
                f"got: {ranked_src}\n"
                f"Hit: {hit}, Reciprocal Rank (rr): {rr:.3f}\n"
            )

        dt = time.time() - t0
        self.stdout.write(self.style.SUCCESS(
              f"\nDone in {dt:.2f}s | k={k}\n"
              f"Recall@{k}: {mean(recalls):.3f} | MRR@{k}: {mean(mrrs):.3f} "
              f"{len(data)} c√¢u."
        ))
```
* Test th·ª≠:
```bash
python manage.py eval_retrieval
```
Output:
```
...
[101] Q: C·ª≠a ph·ª• c√≥ c·∫ßn l·ª±a ch·ªçn theo tu·ªïi gia ch·ªß kh√¥ng?
gold: ['phong_thuy_thuc_hanh_trong_xay_dung_va_kien_truc_nha_o.pdf']
got: ['tu-vi-dau-so-toan-thu-tran-doan.pdf', 'TU_VI_THUC_HANH.pdf', 'phong_thuy_toan_tap.pdf', 'fengshui_phong_thuy_toan_tap.pdf']
Hit: False, Reciprocal Rank (rr): 0.000

Done in 4.93s | k=4
Recall@4: 0.337 | MRR@4: 0.097 101 c√¢u.
```
### 3. T·∫°o l·ªánh ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi d·ª±a tr√™n Lexical F1 (t√πy ch·ªçn + LLM judge mini)
* T·∫°o file copilot/rag/eval_answer.py
* Vi·∫øt h√†m tokenize ƒë∆°n gi·∫£n chuy·ªÉn c√¢u th√†nh t·∫≠p c√°c t·ª´:
```python
def tokenize(s: str) -> list[str]:
    return re.findall(r"[0-9A-Za-z√Ä-·ªπ]+", (s or "").strip())
```
* Vi·∫øt h√†m t√≠nh F1 score:
```python
def f1_score(pred: str, ref: str) -> float:
    p = tokenize(pred)
    r = tokenize(ref)
    if not p or not r:
        return 0.0

    p_set = set(p)
    r_set = set(r)
    overlap = len(p_set & r_set)

    if overlap == 0:
        return 0.0

    precision = overlap / len(p_set)
    recall = overlap / len(r_set)
    return 2 * (precision * recall) / (precision + recall)
```
* T·∫°o prompt cho LLM ƒë√°nh gi√° (ch√∫ √Ω ph·∫£i "{{" ch·ª© kh√¥ng ph·∫£i "{")
```
JUDGE_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "B·∫°n l√† chuy√™n gia, gi√°m kh·∫£o v·ªÅ huy·ªÅn h·ªçc. Cho ƒëi·ªÉm 0..1 v·ªÅ ƒê·ªò CH√çNH X√ÅC so v·ªõi c√¢u tham chi·∫øu. "
     "Ch·ªâ ch·∫•m ƒë·ªô ƒë√∫ng (kh√¥ng ch·∫•m vƒÉn phong). Tr·∫£ v·ªÅ ƒë√∫ng JSON: "
     '{{"score": <float>, "rationale": "<ng·∫Øn g·ªçn>"}}'),
    ("human",
     "C√¢u h·ªèi: {question}\nTham chi·∫øu: {ref}\nTr·∫£ l·ªùi: {pred}\n"
     "Ch·∫•m ƒëi·ªÉm v√† gi·∫£i th√≠ch ng·∫Øn.")
])
```
* T·∫°o l·ªánh:
```python
class Command(BaseCommand):
    help = "ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi: F1 lexical + (tu·ª≥ ch·ªçn) LLM judge mini."

    def add_arguments(self, parser):
        parser.add_argument("--file", default=str(DATA_DIR / "eval" / "qa.jsonl"))
        parser.add_argument("--k", type=int, default=TOP_K)
        parser.add_argument("--model", default=LLM_MODEL)
        parser.add_argument("--judge", action="store_true",
                            help="B·∫≠t ch·∫•m ƒëi·ªÉm b·∫±ng LLM")

    def handle(self, *args, **opts):
        eval_path = Path(opts["file"])
        k = opts["k"]
        model = opts["model"]
        use_judge = opts["judge"]

        data = list(read_jsonl(eval_path))
        if not data:
            raise CommandError(f"Eval set r·ªóng: {eval_path}")

        retriever = get_retriever(k)
        llm = ChatOllama(model=model)

        f1s, judge_scores = [], []
        t0 = time.time()

        for i, item in enumerate(data):
            q = item["q"]
            ref = item.get("ref", "").strip()

            docs = retriever.invoke(q)

            ctx_lines = []

            for j, d in enumerate(docs):
                snippet = d.page_content.strip().replace("\n", " ")

                # if len(snippet) > 500:
                #     snippet = snippet[:500] + "..."
                src = d.metadata.get("source", "")
                ctx_lines.append(f"[{j + 1}] {snippet} (SOURCE: {src})")

            context = "\n\n".join(ctx_lines) if ctx_lines else "(Kh√¥ng c√≥ ng·ªØ c·∫£nh)"

            pred = (ANSWER_PROMPT | llm).invoke({
                "question": q,
                "context": context
            }).content.strip()

            f1 = f1_score(pred, ref)
            f1s.append(f1)
            line = f"[{i+1}] Q: {q}\n REF: {ref}\n PRED: {pred}\n F1: {f1:.3f}"

            if use_judge:
                judge = (JUDGE_PROMPT | llm).invoke({
                    "question": q,
                    "ref": ref,
                    "pred": pred
                }).content.strip()

                m = re.search(r"\{.*\}", judge, re.DOTALL)
                if m:
                    try:
                        score = float(json.loads(m.group(0))["score"])
                    except Exception:
                        score = 0.0

                judge_scores.append(score)
                line += f" | Judge: {score:.3f} ({judge})"

            self.stdout.write(line + "\n")

        dt = time.time() - t0
        summary = f"\nDone in {dt:.2f}s | k={k}\n Avg F1: {mean(f1s):.3f}"
        if use_judge and judge_scores:
            summary += f" | Avg Judge: {mean(judge_scores):.3f}"
        self.stdout.write(self.style.SUCCESS(summary))
```
* Test th·ª≠:

Ph·∫ßn n√†y n·∫øu m√°y ai kh√¥ng ƒë·ªß t√†i nguy√™n c√≥ th·ªÉ x√≥a b·ªõt n·ªôi dung trong file qa.jsonl ƒë·ªÉ gi·∫£m s·ªë c√¢u h·ªèi.
  * 
    ```bash
    python manage.py eval_answer
    ```
    Output:
    ```
    [1] Q: Quy lu·∫≠t T∆∞∆°ng Sinh trong Ng≈© H√†nh di·ªÖn ra theo th·ª© t·ª± n√†o?
     REF: Th·ªßy sinh M·ªôc; M·ªôc sinh H·ªèa; H·ªèa sinh Th·ªï; Th·ªï sinh Kim; Kim sinh Th·ªßy.
     PRED: Quy lu·∫≠t T∆∞∆°ng Sinh trong Ng≈© H√†nh di·ªÖn ra theo th·ª© t·ª±: Th·ªßy sinh M·ªôc, H·ªèa sinh Th·ªï, Th·ªï sinh Kim, Kim sinh Th·ªßy.
    
    Ngu·ªìn:
    - phong_thuy_toan_tap.pdf
    - TU_VI_THUC_HANH.pdf
    - tu-vi-dau-so-toan-thu-tran-doan.pdf
     F1: 0.293
    [2] Q: Ng≈© h√†nh t∆∞∆°ng kh·∫Øc theo th·ª© t·ª± n√†o?
     REF: Th·ªßy kh·∫Øc H·ªèa; H·ªèa kh·∫Øc Kim; Kim kh·∫Øc M·ªôc; M·ªôc kh·∫Øc Th·ªï; Th·ªï kh·∫Øc Th·ªßy.
     PRED: Ng≈© h√†nh t∆∞∆°ng kh·∫Øc theo th·ª© t·ª±: Kim kh·∫Øc H·ªèa, Th·ªßy kh·∫Øc H·ªèa, M·ªôc kh·∫Øc Th·ªï, Th·ªï kh·∫Øc Th·ªßy, H·ªèa kh·∫Øc Kim, M·ªôc kh·∫Øc Kim.
    
    Ngu·ªìn:
    - TU_VI_THUC_HANH.pdf
    - tu-vi-dau-so-toan-thu-tran-doan.pdf
    - fengshui_phong_thuy_toan_tap.pdf
    - phong_thuy_thuc_hanh_trong_xay_dung_va_kien_truc_nha_o.pdf
     F1: 0.261
    ...
    ```
  *
    ```bash
    python manage.py eval_answer --judge
    ```
    Output:
    ```
    [1] Q: Quy lu·∫≠t T∆∞∆°ng Sinh trong Ng≈© H√†nh di·ªÖn ra theo th·ª© t·ª± n√†o?
     REF: Th·ªßy sinh M·ªôc; M·ªôc sinh H·ªèa; H·ªèa sinh Th·ªï; Th·ªï sinh Kim; Kim sinh Th·ªßy.
     PRED: Quy lu·∫≠t T∆∞∆°ng Sinh trong Ng≈© H√†nh di·ªÖn ra theo th·ª© t·ª±: Kim sinh Th·ªßy, Th·ªßy sinh M·ªôc, M·ªôc sinh H·ªèa, H·ªèa sinh Th·ªï, Th·ªï sinh Kim.
    
    Ngu·ªìn: phong_thuy_toan_tap.pdf, TU_VI_THUC_HANH.pdf, tu-vi-dau-so-toan-thu-tran-doan.pdf
     F1: 0.293 | Judge: 0.400 ({"score": 0.4, "rationale": "Danh s√°ch th·ª© t·ª± trong quy lu·∫≠t T∆∞∆°ng Sinh ƒë∆∞·ª£c ƒë∆∞a ra l√† ch√≠nh x√°c nh∆∞ng kh√¥ng theo tr√¨nh t·ª± v√≤ng tr√≤n Ng≈© H√†nh (Kim, Th·ªßy, M·ªôc, H·ªèa, Th·ªï). Tr√¨nh t·ª± v√≤ng tr√≤n Ng≈© H√†nh th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ mi√™u t·∫£ c√°c m·ªëi quan h·ªá v√† quy lu·∫≠t c·ªßa Ng≈© H√†nh, v√¨ v·∫≠y m·ªôt danh s√°ch th·ª© t·ª± theo ƒë√∫ng v√≤ng tr√≤n c√≥ th·ªÉ gi√∫p ng∆∞·ªùi ƒë·ªçc d·ªÖ d√†ng nh·∫≠n bi·∫øt h∆°n v·ªÅ s·ª± li√™n k·∫øt gi·ªØa c√°c y·∫øu t·ªë trong Ng≈© H√†nh."})
    
    Done in 77.75s | k=4
     Avg F1: 0.293 | Avg Judge: 0.400
    ```
* Ch√∫ng ta c√≥ th·ªÉ d·ª±a v√†o c√°c con s·ªë n√†y ƒë·ªÉ ƒë√°nh gi√° v√† c·∫£i thi·ªán h·ªá th·ªëng RAG c·ªßa m√¨nh, v√≠ d·ª•:
  * N·∫øu Recall@k th·∫•p, c√≥ th·ªÉ do t√†i li·ªáu kh√¥ng ƒë·ªß ho·∫∑c qu√° tr√¨nh embedding/retrieval ch∆∞a t·ªët.
  * N·∫øu F1 th·∫•p, c√≥ th·ªÉ do prompt ch∆∞a t·ªët ho·∫∑c LLM ch∆∞a hi·ªÉu ƒë√∫ng ng·ªØ c·∫£nh.
  * D·ª±a v√†o c√°c c√¢u h·ªèi c·ª• th·ªÉ m√† h·ªá th·ªëng tr·∫£ l·ªùi sai ƒë·ªÉ ƒëi·ªÅu ch·ªânh prompt, th√™m t√†i li·ªáu, ho·∫∑c tinh ch·ªânh tham s·ªë:
    * TƒÉng k
    * Thay ƒë·ªïi chunk size/overlap
    * Thay embedding model

## LLM Provider linh ho·∫°t
* V·∫•n ƒë·ªÅ: nhi·ªÅu n∆°i g·ªçi LLM (structured_qa, rag_ask, eval_answer, qa_graph). N·∫øu ƒë·ªïi provider (Ollama ‚Üî OpenRouter), s·ª≠a tay t·ª´ng file s·∫Ω d·ªÖ l·ªói.
* Gi·∫£i ph√°p: t·∫°o m·ªôt factory nh·ªè get_chat(...) tr·∫£ v·ªÅ model ƒë√£ c·∫•u h√¨nh s·∫µn d·ª±a tr√™n .env. M·ªçi l·ªánh ch·ªâ from ... import get_chat v√† d√πng.
* L·ª£i √≠ch: DRY, ƒë·ªïi provider b·∫±ng s·ª≠a .env, kh√¥ng ch·∫°m code nghi·ªáp v·ª• (RAG/graph gi·ªØ nguy√™n).
* Ph·∫°m vi: ch·ªâ Chat model cho sinh c√¢u tr·∫£ l·ªùi/ch·∫•m ƒëi·ªÉm. Embeddings & Chroma v·∫´n d√πng Ollama nh∆∞ B√†i 3 (kh√¥ng ƒë·ªïi).
* B·∫°n c√≥ th·ªÉ xem th·ªëng k√™ s·ª≠ d·ª•ng trong trang activity c·ªßa OpenRouter (n·∫øu d√πng OpenRouter l√†m provider).
![Xem th·ªëng k√™ s·ª≠ d·ª•ng trong trang activity c·ªßa OpenRouter](images/activity_dashboard_in_openrouter.jpeg)
Th·ª±c h√†nh ‚Äî B·∫≠t switch Ollama/OpenRouter
1) C√†i g√≥i (n·∫øu ch∆∞a)
pip install -U langchain-openai openai langchain-ollama

2) C·∫≠p nh·∫≠t .env
* Ch√∫ √Ω: sonoma-sky-alpha (OpenRouter) t√¥i d√πng l√∫c n√†y ch·ªâ mi·ªÖn ph√≠ trong th·ªùi gian nh·∫•t ƒë·ªãnh, c√°c b·∫°n c√≥ th·ªÉ t·ª± t√¨m 
ki·∫øm model kh√°c ph√π h·ª£p.
```
# Ch·ªçn 1:
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b

# Ho·∫∑c:
LLM_PROVIDER=openrouter
LLM_MODEL=openrouter/sonoma-sky-alpha
OPENROUTER_API_KEY=or-xxxxxxxxxxxxxxxx
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_HTTP_REFERER=http://localhost:8000
OPENROUTER_APP_TITLE=fengshui-copilot-dev
```
3) T·∫°o factory: copilot/llm/provider.py
* Th√™m v√†o llm/__init__.py
```python
def env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name, default)
    return v.strip() if isinstance(v, str) else v
```

Ch·ªâ th√™m 1 file nh·ªè ƒë·ªÉ tr√°nh l·∫∑p code; kh√¥ng ƒë·ª•ng g√¨ t·ªõi RAG.
```python
# copilot/llm/provider.py
class ProviderError(RuntimeError): ...

def get_chat(temperature: float = 0.0):
    """
    Tr·∫£ v·ªÅ Chat model ƒë√£ c·∫•u h√¨nh theo .env:
      - LLM_PROVIDER=ollama -> ChatOllama(model)
      - LLM_PROVIDER=openrouter -> ChatOpenAI(base_url=OpenRouter)
      - (tu·ª≥ ch·ªçn) LLM_PROVIDER=openai -> ChatOpenAI (OpenAI g·ªëc)
    """
    provider = (env("LLM_PROVIDER", "ollama") or "ollama")
    model = model or env("LLM_MODEL", "llama3.1:8b")

    if provider == "ollama":
        return ChatOllama(model=model, temperature=temperature)

    if provider == "openrouter":
        api_key = env("OPENROUTER_API_KEY")
        if not api_key:
            raise ProviderError("Thi·∫øu OPENROUTER_API_KEY trong .env")
        base_url = env("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        headers = {
            "HTTP-Referer": env("OPENROUTER_HTTP_REFERER", "http://localhost"),
            "X-Title": env("OPENROUTER_APP_TITLE", "fengshui-copilot-dev"),
        }
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            api_key=api_key,
            base_url=base_url,
            default_headers=headers,
        )

    if provider == "openai":
        api_key = env("OPENAI_API_KEY")
        if not api_key:
            raise ProviderError("Thi·∫øu OPENAI_API_KEY trong .env")
        return ChatOpenAI(model=model, temperature=temperature, api_key=api_key)

    raise ProviderError(f"LLM_PROVIDER kh√¥ng h·ªó tr·ª£: {provider}")
```
4) S·ª≠a 4 l·ªánh ƒë·ªÉ d√πng factory (patch m·∫´u)
```
copilot/management/commands/structured_qa.py
- from langchain_ollama import ChatOllama
+ from ...llm.provider import get_chat
...
- llm = ChatOllama(model=model, temperature=temp)
+ llm = get_chat(temperature=temp)

copilot/management/commands/rag_ask.py
- from langchain_ollama import ChatOllama
+ from ...llm.provider import get_chat
...
- llm = ChatOllama(model=model, temperature=temp)
+ llm = get_chat(temperature=temp)

copilot/management/commands/eval_answer.py
- from langchain_ollama import ChatOllama
+ from ...llm.provider import get_chat
...
- llm = ChatOllama(model=model, temperature=0)
+ llm = get_chat(temperature=0)
...
- judge_raw = (JUDGE_PROMPT | llm).invoke(...)
+ judge_raw = (JUDGE_PROMPT | get_chat(temperature=0)).invoke(...)
```

L∆∞u √Ω: kh√¥ng ƒë·ª•ng B√†i 3 (ingest/retriever) ‚Äî embeddings v·∫´n l√† OllamaEmbeddings(nomic-embed-text).

5) Ki·ªÉm th·ª≠ nhanh (switch b·∫±ng .env, kh√¥ng s·ª≠a code)
```bash
python manage.py rag_ask --q "Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p m·ªánh n√†o?"
```

T√¥i c√≥ th·ª≠ ch·∫°y l·∫°i l·ªánh `python manage.py eval_answer --judge` v·ªõi OpenRouter (sonoma-sky-alpha), k·∫øt qu·∫£ ƒë∆∞·ª£c ƒë∆∞·ª£c l∆∞u 
v√†o file res_1.txt.

# Tri·ªÉn khai l·∫°i m·ª•c ingest:
M·ª•c n√†y ƒë∆∞·ª£c t·∫°o ra v√¨ l√∫c tr∆∞·ªõc th·∫≠t ra ·ªü h√†m _make_id() thu·ªôc file ingest.py t√¥i c√≥ s∆° su·∫•t ghi start_idx thay v√¨ start_index, 
khi·∫øn t√¥i l·∫ßm t∆∞·ªüng r·∫±ng ph·∫ßn ingest kh√¥ng qu√° n·∫∑ng (v√¨ l·∫•y start_idx - kh√¥ng c√≥ n√™n m·∫∑c ƒë·ªãnh l√† None ‚Üí tr√πng id ‚Üí √≠t chunk). 

L√∫c sau ch·ªânh s·ª≠a l·∫°i cho ƒë√∫ng th√¨ th·∫•y ph·∫ßn m√°y kh√¥ng ch·ªãu ƒë∆∞·ª£c n√™n t·ª´ m·ª•c n√†y ch√∫ng ta s·∫Ω tri·ªÉn khai l·∫°i nh∆∞ sau:
* Embedding: d√πng Hugging Face (m·∫∑c ƒë·ªãnh Endpoint API BAAI/bge-m3))
* Vector database: chuy·ªÉn sang Supabase (Postgres + pgvector) v·ªõi LangChain SupabaseVectorStore (v√¨ m√°y t√¥i y·∫øu + project 
ch√∫ng ta l√†m theo h∆∞·ªõng production ‚Üí Supabase ƒë∆∞·ª£c khuy·∫øn ngh·ªã l√† ph√π h·ª£p h∆°n).
* Embedding model: BAAI/bge-m3, 1024 chi·ªÅu (ph√π h·ª£p ti·∫øng Vi·ªát). Tham kh·∫£o th√™m [t·∫°i ƒë√¢y](https://huggingface.co/BAAI/bge-m3).

## B∆∞·ªõc 1: T·∫°o b·∫£ng v√† function trong Supabase
* ·ªû b∆∞·ªõc n√†y b·∫°n h√£y t·∫°o m·ªôt project Supabase fengshui-copilot t·∫°i https://supabase.com/ (n·∫øu ch∆∞a c√≥).
* Sau ƒë√≥ trong Supabase dashboard ‚Üí SQL Editor ‚Üí Ch·ªçn Quickstarts "Langchain" ·ªü m·ª•c Community, l√∫c ƒë√≥ m·ªôt ƒëo·∫°n SQL query 
ƒë∆∞·ª£c t·∫°o ra nh·∫±m t·∫°o b·∫£ng v√† function c·∫ßn thi·∫øt cho LangChain SupabaseVectorStore.
![Quickstarts trong Supabase](images/quickstarts_supabase.jpeg)
* Ch√∫ng ta s·∫Ω ch·ªânh s·ª≠a l·∫°i ƒëo·∫°n query n√†y m·ªôt ch√∫t:
```sql
-- B·∫≠t pgvector extension (n·∫øu ch∆∞a)
create extension if not exists vector;

-- T·∫°o b·∫£ng documents
create table if not exists documents (
    id bigserial primary key, -- bigint, serial nghƒ©a l√† auto-increment
    uid text unique, -- id do ta t·ª± t·∫°o, tr√°nh tr√πng l·∫∑p
    content text, -- t∆∞∆°ng ·ª©ng v·ªõi Document.page_content
    metadata jsonb, -- json binary, hi·ªáu qu·∫£ h∆°n json th√¥ng th∆∞·ªùng, t∆∞∆°ng ·ª©ng v·ªõi Document.metadata
    embedding vector(1024), -- bge-m3: 1024 dims
);

-- T√¨m ki·∫øm consine (tr·∫£ similarity 0..1)
create or replace function match_documents(
    filter jsonb default '{}'::jsonb, -- b·ªô l·ªçc metadata, v√≠ d·ª• {"source": "file.pdf"} ƒë·ªÉ ch·ªâ t√¨m trong file.pdf
    match_count int default 4, -- top-k
    query_embedding vector(1024) default NULL -- embedding c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
) returns table (
    id bigint,
    uid text,
    content text,
    metadata jsonb,
    embedding vector(1024),        -- <‚Äî QUAN TR·ªåNG: tr·∫£ v·ªÅ vector d√πng cho MMR
    similarity double precision
) language sql stable as $$ -- h√†m vi·∫øt b·∫±ng SQL thu·∫ßn (kh√¥ng PL/pgSQL) 
    -- "stable" nghƒ©a l√† k·∫øt qu·∫£ kh√¥ng thay ƒë·ªïi n·∫øu input gi·ªëng (t·ªëi ∆∞u cache)
    select
        d.id,
        d.uid,
        d.content,
        d.metadata,
        d.embedding,
        1 - (d.embedding <=> query_embedding) as similarity -- cosine simlarity (1 - distance), 0..2
    from documents as d
    where d.metadata @> filter -- @>: contains
    order by d.embedding <=> query_embedding -- to√°n t·ª≠ pgvector cho consine distance
    limit match_count;
$$;

-- T·∫°o index ƒë·ªÉ tƒÉng t·ªëc t√¨m ki·∫øm
create index if not exists documents_embedding_idx 
    -- Inverted file with flat: thu·∫≠t to√°n approximate nearest neighbor (ANN) t·ª´ pgvector, nhanh cho vector search l·ªõn.
    -- Th·ª≠ t√¨m hi·ªÉu th√¨ c√≥ v·∫ª gi·ªëng k-means.
    on documents using ivfflat (embedding vector_cosine_ops)
    -- index tr√™n c·ªôt embedding, d√πng to√°n t·ª≠ vector_cosine_ops ƒë·ªÉ t·ªëi ∆∞u cho consine distance.
    with (lists = 100); -- S·ªë c·ª•m trong ivfflat, c√†ng l·ªõn c√†ng ch√≠nh x√°c nh∆∞ng ch·∫≠m h∆°n. Ch√∫ng ta s·∫Ω xem x√©t l·∫°i sau.
    -- Quy t·∫Øc: lists ‚âà sqrt(N) (N = s·ªë vectors), ho·∫∑c 1-4% c·ªßa N. V√≠ d·ª•: N=10K ‚Üí lists=100 (sqrt(10K)=100).
```

## B∆∞·ªõc 2: C·∫•u h√¨nh m√¥i tr∆∞·ªùng
* B·ªï sung c√°c bi·∫øn m√¥i tr∆∞·ªùng trong .env:
```
# Embeddings (Hugging Face)
EMBED_PROVIDER=hf_endpoint     # ho·∫∑c: hf_endpoint (n·∫øu b·∫°n c√≥ Endpoint/TEI ri√™ng)
EMBEDDING_MODEL=BAAI/bge-m3
HUGGINGFACEHUB_API_TOKEN=hf_xxx...

# Supabase (server-side ONLY)
SUPABASE_URL=https://xxxxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOi...
SUPABASE_TABLE=documents
SUPABASE_QUERY_NAME=match_documents
```

## B∆∞·ªõc 3: T·∫°o factory embeddings (gi·ªëng get_chat() ·∫•y)
* HuggingFaceInferenceAPIEmbeddings / HuggingFaceEndpointEmbeddings ƒë∆∞·ª£c LangChain recommend cho Inference API/Endpoint. 
Supabase Python client kh·ªüi t·∫°o b·∫±ng create_client(url, key).
* T·∫°o file copilot/llm/embeddings.py
```python
class EmbeddingProviderError(RuntimeError):...


def get_embeddings():
    provider = env("EMBED_PROVIDER", "hf_endpoint")
    model = env("EMBEDDING_MODEL", "BAAI/bge-m3")

    if provider == "ollama":
        return OllamaEmbeddings(model=model)

    if provider == "hf_inference":
        api_key = env("HUGGINGFACEHUB_API_TOKEN")
        if not api_key:
            raise EmbeddingProviderError("Thi·∫øu HUGGINGFACEHUB_API_TOKEN trong .env")
        return HuggingFaceInferenceAPIEmbeddings(api_key=api_key, model_name=model)

    if provider == "hf_endpoint":
        api_key = env("HUGGINGFACEHUB_API_TOKEN")
        if not api_key:
            raise EmbeddingProviderError("Thi·∫øu HUGGINGFACEHUB_API_TOKEN trong .env")
        return HuggingFaceEndpointEmbeddings(
            model=model,
            task="feature-extraction",
            huggingfacehub_api_token=api_key
        )

    raise EmbeddingProviderError(f"EMBEDDING_PROVIDER kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {provider}")
```

## B∆∞·ªõc 4: Chuy·ªÉn Vector Store sang Supabase
* Chuy·ªÉn rag th√†nh module, t·∫°o file supa.py:
```python
def get_supabase_client() -> client:
    global _SUPABASE_CLIENT
    if _SUPABASE_CLIENT is not None:
        return _SUPABASE_CLIENT

    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
    if not url or not key:
        raise RuntimeError("Thi·∫øu SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY trong .env")

    _SUPABASE_CLIENT = create_client(url, key)
    return _SUPABASE_CLIENT


def get_supabase_table_name() -> str:
    return os.getenv("SUPABASE_TABLE", "documents")


def get_supabase_query_name() -> str:
    # t√™n function t√¨m ki·∫øm (match_documents) b·∫°n ƒë√£ t·∫°o trong DB
    return os.getenv("SUPABASE_QUERY_NAME", "match_documents")
```
* D√πng SupabaseVectorStore + MMR cho retriever.py:
```python
def get_vectorstore():
    supa_client = get_supabase_client()
    embeddings = get_embeddings()
    table = os.getenv("SUPABASE_TABLE", "documents")
    query = os.getenv("SUPABASE_QUERY_NAME", "match_documents")
    return SupabaseVectorStore(client=supa_client, embedding=embeddings, table_name=table, query_name=query)

def get_retriever(top_k: int | None = None):
    vs = get_vectorstore()

    return vs.as_retriever(
        search_type="mmr",
        search_kwargs={"k": top_k, "fetch_k": max(20, 5 * top_k)}
    )
```
* T·∫°o th√™m h√†m sanitize_text() ƒë·ªÉ l√†m s·∫°ch text (b·ªè k√Ω t·ª± kh√¥ng in ƒë∆∞·ª£c):
```python
# V·ªá sinh, lo·∫°i b·ªè control char kh√¥ng mong mu·ªën
def sanitize_text(s: str) -> str:
    if not s:
        return ""
    # chu·∫©n ho√° xu·ªëng 1 kho·∫£ng tr·∫Øng v·ªõi control char; strip cho g·ªçn
    s = _CONTROL_BAD.sub(" ", s)
    return s.strip()
```
* Ch·ªânh ingest th√†nh theo batch:
```python
def ingest_to_supabase(chunks: List[Document]) -> Tuple[int, int]:
    """
    Idempotent ingest:
    - V·ªõi m·ªói source: l·∫•y danh s√°ch uid ƒëang c√≥ trong DB.
    - T·∫°o uid hi·ªán t·∫°i t·ª´ chunks.
      * new = current_uids - db_uids  -> ch·ªâ embed + upsert cho ph·∫ßn n√†y.
      * stale = db_uids - current_uids -> delete ƒë·ªÉ l√†m s·∫°ch.
    - Kh√¥ng ki·ªÉm tra n·ªôi dung thay ƒë·ªïi (kh√¥ng checksum).
    """
    embeds = get_embeddings()
    supa_client = get_supabase_client()
    table = get_supabase_table_name()

    # Ki·ªÉm tra metadata "source", "start_index"
    # for doc in chunks:
    #     print(f"[INGEST] {doc.metadata.get('source', '')} (start={doc.metadata.get('start_index', 0)})")

    by_src: Dict[str, List[Document]] = defaultdict(list)
    for doc in chunks:
        by_src[doc.metadata.get("source", "")].append(doc)

    # Ki·ªÉm tra c√°c ngu·ªìn
    print(by_src.keys())

    total_new, total_delete = 0, 0

    for src, docs in by_src.items():
        res = supa_client.table(table).select("uid").contains("metadata", {"source": src}).execute()
        db_uids = set([row["uid"] for row in (res.data or [])])

        cur_pairs = [(_make_uid(d), d) for d in docs]
        current_uids = set([uid for uid, _ in cur_pairs])

        # Xo√° ‚Äústale‚Äù (nh·ªØng uid ƒëang c√≥ trong DB nh∆∞ng kh√¥ng c√≤n xu·∫•t hi·ªán ·ªü l·∫ßn ingest n√†y)
        stale = list(db_uids - current_uids)
        if stale:
            supa_client.table(table).delete().in_("uid", stale).execute()
            total_delete += len(stale)

        # Ch·ªâ embed + upsert nh·ªØng c√°i m·ªõi
        new_pairs = [(uid, d) for uid, d in cur_pairs if uid not in db_uids]
        if not new_pairs:
            continue

        content = [d.page_content for _, d in new_pairs]
        vectors = embeds.embed_documents(content)

        rows = []
        for (uid, d), vec in zip(new_pairs, vectors):
            rows.append({
                "uid": uid,
                "content": sanitize_text(d.page_content),
                "metadata": d.metadata,
                "embedding": vec
            })

        # Upsert theo batch ƒë·ªÉ tr√°nh payload qu√° l·ªõn
        BATCH_SIZE = 128
        for i in range(0, len(rows), BATCH_SIZE):
            supa_client.table(table).upsert(
                rows[i:i + BATCH_SIZE],
                on_conflict="uid"
            ).execute()

        total_new += len(new_pairs)

    return total_new, total_delete
```
* T·∫°o RPC (Remote Procedure Call) trong Supabase ƒë·ªÉ reset t·ª´ code:
```sql
create or replace function reset_documents()
returns void
language plpgsql
security definer -- ƒë√¢y l√† t√πy ch·ªçn an ninh thi·∫øt l·∫≠p r·∫±ng h√†m s·∫Ω ch·∫°y v·ªõi quy·ªÅn c·ªßa ng∆∞·ªùi t·∫°o h√†m ch·ª© kh√¥ng ph·∫£i ng∆∞·ªùi g·ªçi
as $$
begin
    truncate table documents restart identity;
end;
$$;
```
* B·∫°n c√≥ th·ªÉ xem c√°c h√†m hi·ªán c√≥ trong Database ‚Üí Functions.
![function_supabase.jpeg](images/function_supabase.jpeg)
* Ch·ªânh t∆∞∆°ng ·ª©ng v·ªõi file ingest_corpus.py:
```python
def handle(self, *args, **opts):
    if opts["reset"]:
        self.stdout.write("Xo√° index c≈©...")
        supa_client = get_supabase_client()
        # table = get_supabase_table_name()
        # supa_client.table(table).delete().neq("uid", None).execute()
        # self.stdout.write(self.style.WARNING(f"ƒê√£ reset b·∫£ng Supabase: {table}"))
        # g·ªçi RPC thay v√¨ delete()
        supa_client.rpc("reset_documents").execute()
        self.stdout.write(self.style.SUCCESS("ƒê√£ TRUNCATE + RESTART IDENTITY cho b·∫£ng documents."))

    t0 = time.time()
    stats = ingest_corpus()
    dt = time.time() - t0

    self.stdout.write(self.style.SUCCESS(
        f"INGEST DONE in {dt:.2f}s | files={stats['files']} "
        f"chunks={stats['chunks']} added={stats['added']} deleted={stats['deleted']}\n"
        f"corpus={stats['corpus_dir']} "
        # f"| chroma={stats['chroma_dir']} | collection={stats['collection']}"
    ))
```
**Test th·ª≠:**
```bash
python manage.py ingest_corpus --reset
python manage.py rag_ask --q "Nh√† h∆∞·ªõng ƒê√¥ng Nam h·ª£p m·ªánh n√†o?"
```
* Ch·∫°y l·∫°i l·ªánh eval_retrieval/eval_answer ƒë·ªÉ xem k·∫øt qu·∫£ th·∫ø n√†o.
```bash
python manage.py eval_retrieval --k 6
python manage.py eval_answer --judge --k 6
```
* K·∫øt qu·∫£ nh·∫≠n ƒë∆∞·ª£c: Recall@6: 0.653 | MRR@6: 0.501 101 c√¢u.
* Ch√∫ √Ω, ·ªü ƒë√¢y, ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng, ƒë√∫ng l√† hi·ªán t·∫°i vi·ªác ingest, retrieval ƒë√£ nh·∫π h∆°n r·∫•t nhi·ªÅu, tuy nhi√™n v·∫•n ƒë·ªÅ v·ªÅ ch·∫•t 
l∆∞·ª£ng truy v·∫•n v·∫´n "k√©m" - n√†y ƒë∆°n thu·∫ßn l√† do t√†i li·ªáu v·ªën c√≥ c·ªßa t√¥i b·ªã m√£ h√≥a ·ªü m·ªôt s·ªë ch·ªó (c√°c b·∫°n c√≥ th·ªÉ v√†o t·∫°i li·ªáu 
copy tr·ª±c ti·∫øp th·ª≠) th√†nh ra khi·∫øn cho retriever kh√¥ng t√¨m ƒë√∫ng ƒë∆∞·ª£c n·ªôi dung c·∫ßn thi·∫øt.
* C√°c b·∫°n c√≥ th·ªÉ v√†o xem th·ª≠ b·∫£ng documents trong Supabase ƒë·ªÉ th·∫•y r·∫±ng, n·ªôi dung m·ª•c content c√≥ nhi·ªÅu ch·ªó b·ªã 
l·ªói font nh∆∞ v·ªën l√† "Ng≈© h√†nh" l·∫°i tr·ªü th√†nh "Ng≈© h{nh".
![vietnamese_text_problem.jpeg](images/vietnamese_text_problem.jpeg)
* C√°c b·∫°n c√≥ th·ªÉ t·ª± t√¨m ki·∫øm t√†i li·ªáu phong th·ªßy b·ªï sung ƒë·ªÉ c√≥ th·ªÉ ki·ªÉm tra r√µ h∆°n ch·∫•t l∆∞·ª£ng h·ªá th·ªëng RAG c·ªßa m√¨nh (t√¥i 
th√¨ xem kƒ© trong file m√£ h√≥a kh√° ƒë∆°n ƒëi·ªáu n√™n t·∫°o h√†m decode ƒë∆°n gi·∫£n th√¥i)
```python
def decode_pdf_text(s: str) -> str:
    if not s:
        return ""
    s = (s.replace("{", "√†").replace("}", "√¢")
         .replace("~", "√£").replace("|", "√°"))
    return s
```
* Final result: ƒêi·ªÉm eval_retrieval th·∫•p (l·ªói t√¥i khi t·∫≠p n√†y d√πng gpt sinh kh√¥ng t·ªët) nh∆∞ng ƒëi·ªÉm judge m·ªÅm m·ªèng h∆°n 
(ch·∫•m b·∫±ng LLM) r·∫•t t·ªët. T√¥i khuy·∫øn ngh·ªã n·∫øu c√≥ th·ªùi gian b·∫°n n√™n t·ª± t·∫°o th·ª≠ b·ªô d·ªØ li·ªáu kh√°c.

# B√†i 5: LangGraph ‚Äì v√≤ng l·∫∑p ‚Äútr·∫£ l·ªùi ‚Üí ch·∫•m ƒëi·ªÉm ‚Üí (n·∫øu k√©m) truy v·∫•n l·∫°i‚Äù
* LangGraph l√† th∆∞ vi·ªán ƒë·ªÉ b·∫°n "v·∫Ω" ƒë·ªì th·ªã tr·∫°ng th√°i cho quy tr√¨nh nhi·ªÅu b∆∞·ªõc v·ªõi LLM:
  * State: 1 dict (ho·∫∑c TypedDict) ch·ª©a c√°c field ch√∫ng ta c·∫ßn
  * Node: 1 h√†m nh·∫≠n state v√† tr·∫£ v·ªÅ ph·∫ßn c·∫≠p nh·∫≠t state
  * Edge: ƒë∆∞·ªùng ƒëi gi·ªØa c√°c node, c√≥ th·ªÉ c·ªë ƒë·ªãnh (A ‚Üí B) ho·∫∑c c√≥ ƒëi·ªÅu ki·ªán (A ‚Üí B/C t√πy d·ªØ li·ªáu trong state)
  * Loop: d√πng c·∫°nh c√≥ ƒëi·ªÅu ki·ªán ƒë·ªÉ quay l·∫°i 1 node tr∆∞·ªõc ƒë√≥ (v√≠ d·ª•: ch·∫•m ƒëi·ªÉm th·∫•p ‚Üí quay l·∫°i truy v·∫•n)
  * Kh√°c v·ªõi chain th∆∞·ªùng 1 chi·ªÅu, graph th√¨ r·∫Ω nh√°nh / loop ƒë∆∞·ª£c n√™n h·ª£p t·ª± ch·∫•m ƒëi·ªÉm (judge) v√† rewrite.

* M·ª•c ti√™u: x√¢y d·ª±ng mini RAG graph c√≥ 5 node:
  1. retrieve: l·∫•y t√†i li·ªáu t·ª´ supabase
  2. grade: grade documents - ƒë√°nh gi√° v√† l·ªçc c√°c t√†i li·ªáu ƒë∆∞·ª£c retrieve
  3. answer: so·∫°n c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh (Prompt c√≥ c·∫•u tr√∫c L√Ω do ‚Üí V√≠ d·ª• ‚Üí K·∫øt lu·∫≠n)
  4. judge: ch·∫•m ƒëi·ªÉm c√¢u tr·∫£ l·ªùi
  5. rewrite_query: n·∫øu ƒëi·ªÉm judge th·∫•p, s·ª≠a c√¢u h·ªèi ƒë·ªÉ truy v·∫•n l·∫°i
* ƒê·ªì th·ªã c√°c node:
```mermaid
graph TD
    START --> A[retrieve]
    A[retrieve] --> B[grade]
    B --> C[answer]
    C --> D[judge]
    D -- retry --> E[rewrite_query]
    E --> A
    D -- good --> END
```

## C√°c b∆∞·ªõc th·ª±c hi·ªán
1) T·∫°o graph: copilot/graph/rag_graph.py
* ƒê·ªãnh nghƒ©a tr·∫°ng th√°i c·ªßa 1 node:
```python
class QAState(TypedDict, total=False):  # total=False ƒë·ªÉ c√°c tr∆∞·ªùng kh√¥ng b·∫Øt bu·ªôc ph·∫£i c√≥, c√≥ th·ªÉ b·ªï sung d·∫ßn
    question: str
    rewritten: str
    context: List[Document]
    answer: str
    k: int  # s·ªë t√†i li·ªáu l·∫•y v·ªÅ
    iterations: int  # s·ªë v√≤ng ƒë√£ l·∫∑p
    verdict: Literal["good", "retry"]  # verdict: ph√°n quy·∫øt
```
* V√¨ t√¥i mu·ªën l√†m th·∫≠t chu·∫©n ch·ªânh n√™n ch√∫ng ta h√£y t·∫°o th√™m c√°c th∆∞ m·ª•c ri√™ng cho t·ª´ng prompt trong 1 package 
copilot/graph/prompts th·ªëng nh·∫•t:
```
prompts/
  __init__.py
  answer_prompt
    __init__.py
    answer_human_prompt.txt
    answer_system_prompt.txt
  grade_prompt
    __init__.py
    grade_human_prompt.txt
    grade_system_prompt.txt
  judge_prompt
    __init__.py
    judge_human_prompt.txt
    judge_system_prompt.txt
  rewrite_prompt
    __init__.py
    rewrite_human_prompt.txt
    rewrite_system_prompt.txt
```
* Trong file __init__.py c·ªßa package prompts, ch√∫ng ta s·∫Ω vi·∫øt h√†m load_prompt() ƒë·ªÉ load prompt t·ª´ file txt:
```python
def load_prompt(package_path: Path, name: str) -> str:
    p = package_path / name
    return p.read_text(encoding="utf-8")
```
* Trong t·ª´ng file __init__.py c·ªßa t·ª´ng package con trong package prompts, ch√∫ng ta s·∫Ω kh·ªüi t·∫°o bi·∫øn prompt t∆∞∆°ng ·ª©ng, v√≠ 
d·ª• trong prompts/answer_prompt/__init__.py:
```python
package_path = Path(__file__).resolve().parent
ANSWER_SYSTEM_PROMPT = load_prompt(package_path, "answer_system_prompt.txt")
ANSWER_HUMAN_PROMPT = load_prompt(package_path, "answer_human_prompt.txt")
```
* ·ªû ph·∫ßn n√†y, t√¥i c≈©ng c√≥ ch·ªânh s·ª≠a l·∫°i h√†m get_chat() 1 ch√∫t ƒë·ªÉ ch√∫ng ta c√≥ th·ªÉ truy·ªÅn t√™n model ri√™ng cho t·ª´ng node:
```python
def get_chat(role: str | None = None, temperature: float = 0.0):
    provider = env("LLM_PROVIDER", "ollama").lower()  # Trong project n√†y th√¨ ch·ªâ d√πng provider chung th√¥i
    model = env(f"{role}_MODEL".upper(), env("LLM_MODEL", "llama3.1:8b")).lower()
    print(f"[LLM] Provider={provider}, Model={model}, Temp={temperature}")
    ...
```
* H√†m get_retriever() c≈©ng ƒë∆∞·ª£c t√¥i th√™m v√†o 1 c√°ch th·ª©c truy v·∫•n kh√°c (ch√∫ng ta c≈©ng c√≥ th·ªÉ th·ª≠ MQR v·ªõi retriever l√† mmr):
```python
def get_retriever(top_k: int = TOP_K):
    vs = get_vectorstore()
    mode = RETRIEVER_MODE
    print("Retriever mode:", mode, "| top_k:", top_k)

    base = vs.as_retriever(
            search_type="similarity",
            search_kwargs={"k": top_k}
        )
    if mode == "mq":
        llm = get_chat("MQR", temperature=0)
        mqr = MultiQueryRetriever.from_llm(retriever=base, llm=llm, include_original=True)

        class _Adapter:
            @staticmethod # ph∆∞∆°ng th·ª©c kh√¥ng c·∫ßn tham chi·∫øu ƒë·∫øn l·ªõp (kh√¥ng truy·ªÅn self)
            def invoke(q: str):
                docs = mqr.invoke(q)
                return docs[:top_k]
        return _Adapter()

    if mode == "similarity":
        return base

    return vs.as_retriever(
        search_type="mmr",
        search_kwargs={"k": top_k, "fetch_k": max(20, 5 * top_k)}
        # search_kwargs={"k": top_k}
    )
```
* B·ªï sung b√™n ph√≠a file .env (t√¥i nghƒ© MQR_MODEL n√™n ƒë·ªÉ m√¥ h√¨nh kh√°c v√¨ th·∫•y m√¥ h√¨nh t√¥i d√πng k√©m h∆°n so v·ªõi x-ai, h√£y 
th·ª≠ ho√°n ƒë·ªïi xem nha):
```
GRADE_MODEL=deepseek/deepseek-chat-v3.1:free
ANSWER_MODEL=x-ai/grok-4-fast:free
JUDGE_MODEL=google/gemini-2.0-flash-exp:free
REWRITE_MODEL=x-ai/grok-4-fast:free
MQR_MODEL=tngtech/deepseek-r1t2-chimera:free
```
* T·∫°o c√°c node:
```python
# ----- Node: retrieve -----
def retrieve_node(state: QAState) -> QAState:
    q = state.get("rewritten", None) or state["question"]
    k = state.get("k", 6)
    retriever = get_retriever(k)
    docs = retriever.invoke(q)
    return {"context": docs}


# ----- Node: grade (l·ªçc t√†i li·ªáu) -----
_GRADER = get_chat("GRADE", temperature=0)

GRADER_PROMPT = ChatPromptTemplate.from_messages([
    ("system", GRADE_SYSTEM_PROMPT),
    ("human", GRADE_HUMAN_PROMPT)
])


def grade_node(state: QAState) -> QAState:
    q = state.get("rewritten", None) or state["question"]
    docs = state["context"]
    kept: List[Document] = []
    grand_chain = GRADER_PROMPT | _GRADER | (lambda x: x.content.strip().upper())
    for d in docs:
        res = grand_chain.invoke({
            "question": q,
            "doc": d
        })
        if res.startswith("Y"):
            kept.append(d)

        if not kept:
            kept = docs[:3]

    return {"context": kept}


# ----- Node: answer -----
_ANSWER_LLM = get_chat("ANSWER", temperature=0.2)  # TƒÉng ƒë·ªô s√°ng t·∫°o m·ªôt ch√∫t

_ANSWER_PROMPT = ChatPromptTemplate.from_messages([
    ("system", ANSWER_SYSTEM_PROMPT),
    ("human", ANSWER_HUMAN_PROMPT)
])


def _format_context(docs: List[Document]) -> str:
    part = []
    for i, d in enumerate(docs):
        snippet = d.page_content.strip().replace("\n", " ")
        src = d.metadata.get("source", "")
        part.append(f"[{i+1}] {snippet} (SOURCE: {src})")

    return "\n\n".join(part) if part else "(Kh√¥ng c√≥ ng·ªØ c·∫£nh)"


def answer_node(state: QAState) -> QAState:
    q = state.get("rewritten", None) or state["question"]
    docs = state["context"]
    context = _format_context(docs)
    answer_chain = _ANSWER_PROMPT | _ANSWER_LLM | (lambda x: x.content.strip())
    res = answer_chain.invoke({
        "question": q,
        "context": context
    })
    return {"answer": res}


# ----- Node: judge -----
_JUDGE_LLM = get_chat("JUDGE", temperature=0)

_JUDGE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", JUDGE_SYSTEM_PROMPT),
    ("human", JUDGE_HUMAN_PROMPT)
])


def judge_node(state: QAState) -> QAState:
    q = state.get("rewritten", None) or state["question"]
    docs = state["context"]
    context = _format_context(docs)
    ans = state["answer"]
    judge_chain = _JUDGE_PROMPT | _JUDGE_LLM | (lambda x: x.content.strip().upper())
    res = judge_chain.invoke({
        "question": q,
        "context": context,
        "answer": ans
    })
    verdict: Literal["good", "retry"] = "good" if res.startswith("G") else "retry"
    return {"verdict": verdict}


# ----- Node: rewrite_query (khi RETRY) -----
_REWRITER = get_chat("REWRITE", temperature=0)

REWRITE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", REWRITE_SYSTEM_PROMPT),
    ("human", REWRITE_HUMAN_PROMPT)
])


def rewrite_node(state: QAState) -> QAState:
    q = state["question"]
    rewriter_chain = REWRITE_PROMPT | _REWRITER | (lambda x: x.content.strip())
    res = rewriter_chain.invoke({
        "question": q
    })
    iters = state.get("iterations", 0) + 1
    return {"rewritten": res, "iterations": iters}
```
* T·∫°o graph:
```python
# ----- Build graph -----
def build_graph(max_iters: int = 2):
    sg = StateGraph(QAState)

    sg.add_node("retrieve", retrieve_node)
    sg.add_node("grade", grade_node)
    sg.add_node("answer", answer_node)
    sg.add_node("judge", judge_node)
    sg.add_node("rewrite_query", rewrite_node)

    sg.add_edge(START, "retrieve")
    sg.add_edge("retrieve", "grade")
    sg.add_edge("grade", "answer")
    sg.add_edge("answer", "judge")

    def should_retry(state: QAState) -> Literal["end", "rewrite"]:
        if state.get("verdict") == "good":
            return "end"
        if state.get("iterations", 0) >= max_iters:
            return "end"
        return "rewrite"

    sg.add_conditional_edges(
        "judge",
        should_retry,
        {
            "end": END,  # K·∫øt th√∫c
            "rewrite": "rewrite_query"
        }
    )
    sg.add_edge("rewrite_query", "retrieve")
    
    # Debug ƒë∆°n gi·∫£n v·ªõi MemorySaver
    memory = MemorySaver()
    app = sg.compile(checkpointer=memory)

    return app
```
* T·∫°o l·ªánh commands/qa_graph.py ƒë·ªÉ ch·∫°y c√°c agents n√†y:
```python
class Command(BaseCommand):
    help = "Generate a QA graph from the database."

    def add_arguments(self, parser):
        parser.add_argument("--q", required=True, help="C√¢u h·ªèi")
        parser.add_argument("--k", type=int, default=TOP_K, help="s·ªë ƒëo·∫°n l·∫•y ·ªü retriever")
        parser.add_argument("--max_iters", type=int, default=2, help="s·ªë v√≤ng t·ªëi ƒëa")

    def handle(self, *args, **opts):
        q = opts["q"]
        k = opts["k"]
        max_iters = opts["max_iters"]

        app, memory = build_graph(max_iters)

        state = {
            "question": q,
            "k": k,
            "iterations": 0
        }

        tid = "cli-" + hashlib.md5(q.encode("utf-8")).hexdigest()[:8]
        cfg = {
            "configurable":
                {
                    "thread_id": tid,
                }
        }

        final = app.invoke(state, config=cfg)
        print("[INVOKE] DONE")
        print(f"Memory:")
        checkpoints = list(memory.list(cfg))
        for cp in checkpoints:
            print(cp)
        print()

        answer = final.get("answer", "").strip()
        verdict = final.get("verdict", "good")
        self.stdout.write(self.style.SUCCESS(f"[{verdict}]\n{answer}"))
```
_Ch√∫ √Ω:_ 

Ngo√†i c√°ch debug tr√™n CLI v·ªõi MemorySaver(), c√°c b·∫°n c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng extension AI Agent Debugger tr√™n PyCharm 
ƒë·ªÉ c√≥ giao di·ªán tr·ª±c quan h∆°n. 

Nh·ªõ r·∫±ng ph·∫£i c·∫•u h√¨nh running tr√™n PyCharm l·ªánh m√† b·∫£n th√¢n mu·ªën ch·∫°y ch·ª© kh√¥ng th·ªÉ ch·∫°y l·ªánh tr·ª±c ti·∫øp tr√™n CLI v√¨ 
extension kia s·∫Ω kh√¥ng th·ªÉ ph√°t hi·ªán ƒë∆∞·ª£c ti·∫øn tr√¨nh.
* Ch·∫°y th·ª≠ l·ªánh:
```bash
python manage.py qa_graph --q "M·ªánh Kim h·ª£p m√†u g√¨?" 
```
Ta ƒë∆∞·ª£c output sau (l·∫∑p ƒë√∫ng 2 l·∫ßn ƒë·ªÉ ra k·∫øt qu·∫£ t·ªët, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·ª≠ ph·ªëi h·ª£p c√°c m√¥ h√¨nh theo ki·ªÉu kh√°c th·ª≠)
```
Server listening on 127.0.0.1:54105
Client connected from ('127.0.0.1', 54107)
[LLM] Provider=openrouter, Model=deepseek/deepseek-chat-v3.1:free, Temp=0
[LLM] Provider=openrouter, Model=x-ai/grok-4-fast:free, Temp=0.2
[LLM] Provider=openrouter, Model=google/gemini-2.0-flash-exp:free, Temp=0
[LLM] Provider=openrouter, Model=x-ai/grok-4-fast:free, Temp=0
Retriever mode: mq | top_k: 6
[LLM] Provider=openrouter, Model=tngtech/deepseek-r1t2-chimera:free, Temp=0
Retriever mode: mq | top_k: 6
[LLM] Provider=openrouter, Model=tngtech/deepseek-r1t2-chimera:free, Temp=0
[INVOKE] DONE
Memory:
CheckpointTuple...
CheckpointTuple...
CheckpointTuple...
CheckpointTuple...
CheckpointTuple...
CheckpointTuple...

[good]
Trong phong th·ªßy ng≈© h√†nh, m·ªánh Kim ƒë∆∞·ª£c li√™n k·∫øt v·ªõi m√†u tr·∫Øng, t∆∞·ª£ng tr∆∞ng cho s·ª± tinh khi·∫øt v√† kim lo·∫°i s√°ng b√≥ng, gi√∫p c√¢n b·∫±ng nƒÉng l∆∞·ª£ng trong kh√¥ng gian. B·∫°n c√≥ th·ªÉ √°p d·ª•ng m√†u tr·∫Øng cho c√°c v·∫≠t d·ª•ng nh∆∞ t∆∞·ªùng ph√≤ng ho·∫∑c ƒë·ªì trang tr√≠ ƒë·ªÉ h·ªó tr·ª£ m·ªánh Kim, k·∫øt h·ª£p v·ªõi ch·∫•t li·ªáu kim lo·∫°i ƒë·ªÉ tƒÉng c∆∞·ªùng d√≤ng ch·∫£y d∆∞∆°ng. Ngo√†i ra, c√°c m√†u trung t√≠nh nh∆∞ x√°m b·∫°c c≈©ng c√≥ th·ªÉ h·ªó tr·ª£ gi√°n ti·∫øp qua t∆∞∆°ng sinh t·ª´ Th·ªï.

Ngu·ªìn: phong_thuy_toan_tap.pdf (t·ª´ [4]), fengshui_phong_thuy_toan_tap.pdf (t·ª´ [1] v√† [2]).
Client disconnected

Process finished with exit code 0
```
* C√°c event ƒë∆∞·ª£c AI Agent Debuger tracing ƒë∆∞·ª£c:
![ai_agent_debuger_events.png](images/ai_agent_debuger_events.png)
* ƒê·ªì th·ªã m√† AI Agent Debuger t·∫°o d·ª±a tr√™n c√°c node ƒë∆∞·ª£c ph√°t hi·ªán:
![ai_agent_debuger_graph.png](images/ai_agent_debuger_graph.png)

# B√†i 6: Tool Use + Router
* M·ª•c ti√™u: t·∫°o endpoint `POST /api/ask nh·∫≠n {question, k?, mode?, thread_id?}` ch·∫°y rag_graph ·ªü b√†i 5.
_ƒê·∫øn b√†i n√†y t√¥i l·∫°i ƒë·ªïi retriver mode th√†nh mmr v√¨ th·∫•y n√≥ cho k·∫øt qu·∫£ t·ªët h∆°n_
* T·∫°o file copilot/graph/run.py nh·∫±m ch·∫°y graph t·ª´ code (kh√¥ng qua CLI nh∆∞ tr∆∞·ªõc n·ªØa):
```python
def _uniq_sources(docs: List[Document], limit: int = 8):
    seen, out = set(), []
    for d in docs:
        src = d.metadata.get("source", "unknown")
        if src in seen:
            continue
        seen.add(src)
        snippet = (d.page_content or "").strip().replace("\n", " ")
        out.append({"source": src, "snippet": snippet[:200]})
        if len(out) >= limit:
            break
    return out

def run_graph(question: str, k: int = TOP_K, max_iters: int = 2, tid: str | None = None,
              make_thread_id_from_question: bool = False) -> Dict[str, Any]:
    if make_thread_id_from_question:
        tid = "cli-" + hashlib.md5(question.encode("utf-8")).hexdigest()[:8]
    tid = tid or f"web-{uuid4()}"

    app, memory = build_graph(max_iters)

    state = {
        "question": question,
        "k": k,
        "iterations": 0
    }

    cfg = {
        "configurable":
            {
                "thread_id": tid,
            }
    }

    final = app.invoke(state, config=cfg)
    print("[INVOKE] DONE")
    print(f"Memory:")
    checkpoints = list(memory.list(cfg))
    for cp in checkpoints:
        print(cp)
    print()

    answer = final.get("answer", "").strip()
    verdict = final.get("verdict", "good")
    docs: List[Document] = final.get("context", [])
    sources = _uniq_sources(docs)

    return {"thread_id": tid, "answer": answer, "verdict": verdict, "sources": sources}
```
* V√¨ b√™n ph·∫ßn l·ªánh ch·∫°y CLI qa_graph.py b·ªã l·∫∑p code n√™n t√¥i c√≥ ch·ªânh l·∫°i ch√∫t nh∆∞ sau:
```python
    def handle(self, *args, **opts):
        q = opts["q"]
        k = opts["k"]
        max_iters = opts["max_iters"]

        result = run_graph(q, k, max_iters, make_thread_id_from_question=True)

        answer = result.get("answer", "").strip()
        verdict = result.get("verdict", "good")
        self.stdout.write(self.style.SUCCESS(f"[{verdict}]\n{answer}"))
```
* ·ªû ƒë√¢y ch√∫ng ta s·∫Ω tri·ªÉn khai theo ki·∫øn tr√∫c MVT c·ªßa Django. Tr∆∞·ªõc ti√™n s·∫Ω t·∫°o file copilot/views/api.py:
```python
@csrf_exempt
def api_ask(req):
    if req.method != "POST":
        return HttpResponseBadRequest("POST only")

    try:
        data = json.loads(req.body.decode('utf-8'))
        q = str(data['question']).strip()
    except RuntimeError:
        return HttpResponseBadRequest("Missing 'question'")

    k = int(data.get('k', TOP_K))
    max_iters = int(data.get('max_iters', 2))
    thread_id = data.get('thread_id', None)

    try:
        result = run_graph(q, k, max_iters, tid=thread_id, make_thread_id_from_question=True)
        return JsonResponse({"ok": True, **result})
    except Exception as e:
        return JsonResponse({"ok": False, "error": type(e).__name__, "detail": str(e)}, status=500)
```
* T·∫°o ti·∫øp file render c√°c page copilot/views/pages.py:
```python
def page_ask(req):
    return render(req, "ask.html")
```
* T·∫°o template copilot/templates/ask.html (c√°i n√†y t√¥i kh√¥ng vi·∫øt code tr·ª±c ti·∫øp m√† d√πng ChatGPT ƒë·ªÉ t·∫°o, b·∫°n c√≥ th·ªÉ tham kh·∫£o):
```html
<!doctype html>
<html>
<head><meta charset="utf-8"><title>RAG Ask</title></head>
<body>
  <h3>Fengshui Copilot</h3>
  <textarea id="q" rows="3" cols="80" placeholder="Nh·∫≠p c√¢u h·ªèi‚Ä¶"></textarea><br/>
  <button id="btn">H·ªèi</button>
  <pre id="out"></pre>
  <script>
    let thread = null;
    document.getElementById('btn').onclick = async () => {
      const q = document.getElementById('q').value;
      const body = {question: q, k: 6, iters: 2, thread_id: thread};
      const res = await fetch('/api/ask', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(body)});
      const json = await res.json();
      if (json.ok) thread = json.thread_id;
      document.getElementById('out').textContent = JSON.stringify(json, null, 2);
    };
  </script>
</body>
</html>

```
* T·∫°o file copilot/urls.py ƒë·ªÉ ƒë·ªãnh tuy·∫øn URL:
```python
urlpatterns = [
    path("api/ask", api_ask, name="api_ask"),
    path("ask", page_ask, name="page_ask"),
]
```
* Ch·ªânh s·ª≠a file fengshui_copilot/urls.py ƒë·ªÉ include copilot.urls:
```python
urlpatterns = [
    path("admin/", admin.site.urls),
    path("", include("copilot.urls")),
]
```
* T·∫°o file test.http ƒë·ªÉ test /api/ask:
```http request
POST http://127.0.0.1:8000/api/ask
Content-Type: application/json

{
  "question": "M·ªánh Kim h·ª£p m√†u g√¨?",
  "k": 6,
  "max_iters": 2
}
```
Output:
```json
{
  "ok": true,
  "thread_id": "cli-63594dd0",
  "answer": "M·ªánh Kim h·ª£p v·ªõi c√°c m√†u thu·ªôc h√†nh Kim nh∆∞ tr·∫Øng, x√°m ho·∫∑c b·∫°c, v√¨ ch√∫ng t∆∞·ª£ng tr∆∞ng cho s·ª± kh·ªüi ƒë·∫ßu m·ªõi v√† c√≥ th·ªÉ h·ªó tr·ª£ c√¢n b·∫±ng nƒÉng l∆∞·ª£ng. Nh·ªØng m√†u n√†y gi√∫p tƒÉng c∆∞·ªùng t√≠nh ch·∫•t c·ªßa m·ªánh Kim, ƒë·ªìng th·ªùi c√≥ th·ªÉ l√†m d·ªãu b·ªõt c√°c h√†nh kh√°c nh∆∞ H·ªèa ho·∫∑c h·ªó tr·ª£ cho m·ªánh Th·ªßy. Trong phong th·ªßy, vi·ªác ch·ªçn m√†u tr·∫Øng ho·∫∑c b·∫°c cho c√°c v·∫≠t d·ª•ng nh∆∞ xe c·ªô c≈©ng ƒë∆∞·ª£c khuy·∫øn ngh·ªã ƒë·ªÉ mang l·∫°i s·ª± an to√†n v√† h√†i h√≤a.\n\nNgu·ªìn: phong_thuy_toan_tap.pdf, fengshui_phong_thuy_toan_tap.pdf",
  "verdict": "good",
  "sources": [
    {
      "source": "phong_thuy_toan_tap.pdf",
      "snippet": "ch·∫Øc ch·∫Øn r·∫±ng m√†u n√†y kh√¥ng xung kh·∫Øc v·ªõi m√†u Ng≈© h√†nh t∆∞∆°ng ·ª©ng v·ªõi tu·ªïi c·ªßa m√¨nh. V√≠ d·ª•, m·ªôt thanh ni√™n ƒë·∫ßy v·∫ª nam t√≠nh, nh·∫•t l√† tu·ªïi Ng·ªç m·∫°ng H·ªèa, kh√¥ng n√™n ch·ªçn xe m√†u ƒë·ªè v√¨ m√†u n√†y l√†m tƒÉng th√™m"
    },
    {
      "source": "fengshui_phong_thuy_toan_tap.pdf",
      "snippet": "ƒë√≥.   T√≠nh ch·∫•t c·ªßa Ng≈© H√†nh  M√†u l·ª•c (M·ªôc): c√¥ng vi·ªác kinh doanh m·ªõi, s·ª± tƒÉng tr∆∞·ªüng v√† ph√°t tri·ªÉn.  M√†u ƒë·ªè (H·ªèa): nƒÉng ƒë·ªông, s·ªët s·∫Øng v√† x·ªüi l·ªüi, h∆∞·ªõng ƒë·∫øn t∆∞∆°ng lai.  M√†u v√†ng (Th·ªï): tr√≠ tu·ªá, ch·ª´ng"
    }
  ]
}
```
* V√†o ti·∫øp ƒë∆∞·ªùng d·∫´n sau tr√™n web ƒë·ªÉ test giao di·ªán /ask: http://127.0.0.1:8000/ask
![simple_ask_page.png](images/simple_ask_page.png)

# B√†i 7: Streaming
* M·ªôt s·ªë kh√°i ni·ªám:
  * Streaming: server ƒë·∫©y d·∫ßn t·ª´ng ph·∫ßn k·∫øt qu·∫£ (token/phase) v·ªÅ client thay v√¨ ch·ªù xong to√†n b·ªô.
  * SSE (Server-Sent Events): k·ªπ thu·∫≠t push t·ª´ server ƒë·∫øn client qua HTTP, client m·ªü k·∫øt n·ªëi l√¢u d√†i (long-lived connection). 
  D√πng HTTP text/event-stream, client ƒë·ªçc b·∫±ng EventSource. ƒê∆°n gi·∫£n, m·ªôt chi·ªÅu (server ‚Üí client).
  * Django StreamingHttpResponse: cho ph√©p g·ª≠i t·ª´ng chunk d·ªØ li·ªáu v·ªÅ client ngay khi c√≥ s·∫µn. H·ª£p ƒë·ªÉ l√†m SSE.
  * LangChain stream: h·∫ßu h·∫øt chat models ƒë·ªÅu h·ªó tr·ª£ .stream(...) ƒë·ªÉ l·∫•y d·∫ßn token.
* M·ª•c ti√™u: t·∫°o endpoint `POST /api/ask_stream` (SSE) ‚Üí ƒë·∫©y c√°c event:
  * phase: retrieve/grade/answer/judge/rewrite
  * source: t·ª´ng ngu·ªìn
  * kept: t·ª´ng t√†i li·ªáu ƒë∆∞·ª£c gi·ªØ l·∫°i sau grade
  * token: t·ª´ng m·∫©u text c·ªßa c√¢u tr·∫£ l·ªùi
  * final: k·∫øt qu·∫£ cu·ªëi c√πng, verdict, thread_id
## C√°c b∆∞·ªõc th·ª±c hi·ªán
* T·∫°o h√†m h·ªó tr·ª£ ƒë·ªãnh d·∫°ng event SSE:
```python
def _sse(data: str, event: str | None = None) -> str:
    head = f"event: {event}\n" if event else ""
    return f"{head}data:{data}\n\n"  # N·∫øu mu·ªën ch·ªânh s·ª≠a ·ªü ƒë√¢y th√¨ xem x√©t t∆∞∆°ng ·ª©ng b√™n ask.html nha
```
* T·∫°o view SSE: th√™m endpoint m·ªõi trong copilot/views/api.py
```python
@csrf_exempt
def api_ask_stream(req):
    if req.method != "POST":
        return HttpResponseBadRequest("POST only")

    try:
        payload = json.loads(req.body.decode('utf-8'))
        q = str(payload['question']).strip()
    except RuntimeError:
        return HttpResponseBadRequest("Missing 'question'")

    k = int(payload.get('k', TOP_K))
    max_iters = int(payload.get('max_iters', 2))
    thread_id = payload.get('thread_id', None)
    ...
```
* T·∫°o Generator trong h√†m tr√™n ƒë·ªÉ yield events theo format SSE, client nh·∫≠n v√† hi·ªÉn th·ªã realtime:
```python
def event_stream():
    q_out: queue.Queue[tuple[str, str]] = queue.Queue()  # Thread-safe queue ƒë·ªÉ giao ti·∫øp gi·ªØa thread

    def emit(event: str, data):
        q_out.put((event, json.dumps(data) if not isinstance(data, str) else data))

    def run_graph_thread():
        # Kh√¥ng th·ªÉ t√°i s·ª≠ d·ª•ng run_graph v√¨ n√≥ kh√¥ng h·ªó tr·ª£ streaming (kh√¥ng c√≥ "emit" callback)
        try:
            app, memory = build_graph(max_iters)
            tid = thread_id or f"sse-{os.getpid()}"
            print(f"[INVOKE] START thread_id={tid}")

            state = {
                "question": q,
                "k": k,
                "iterations": 0,
                "emit": emit  
              # Vi·ªác th√™m Callable v√†o state s·∫Ω g√¢y l·ªói n·∫øu kh√¥ng ƒëi·ªÅu ch·ªânh do Serializer c·ªßa MemorySaver() do 
              # kh√¥ng th·ªÉ x·ª≠ l√≠ lo·∫°i ƒë·ªëi t∆∞·ª£ng n√†y 
            }

            cfg = {
                "configurable":
                    {
                        "thread_id": tid,
                    }
            }

            final = app.invoke(state, config=cfg)
            print(f"[INVOKE] Thread {tid} DONE")

            answer = final.get("answer", "").strip()
            verdict = final.get("verdict", "good")

            rest = {
                "thread_id": tid,
                "answer": answer,
                "verdict": verdict,
            }

            q_out.put(("final", json.dumps(rest)))
        except Exception as e:
            # ƒë·∫©y l·ªói ra client ƒë·ªÉ b·∫°n th·∫•y ngay trong UI
            q_out.put(("error", json.dumps({"type": type(e).__name__, "msg": str(e)}, ensure_ascii=False)))

        q_out.put(("__done__", ""))
    
    # T·∫°o thread ch·∫°y song song, ch√∫ √Ω ph·∫£i ƒë·∫∑t daemon=True ƒë·ªÉ n√≥ n·∫øu ng·∫Øt ch∆∞∆°ng tr√¨nh th√¨ thread n√†y c≈©ng d·ª´ng
    # theo, n·∫øu kh√¥ng ch∆∞∆°ng tr√¨nh ch·ªâ tho√°t sau khi t·∫•t c·∫£ non-daemon threads k·∫øt th√∫c (ho·∫∑c b·ªã join()).
    t = threading.Thread(target=run_graph_thread, daemon=True)
    t.start()

    while True:
        ev, data = q_out.get()
        if ev == "__done__":
            break
        yield _sse(data, ev)
```
* Tr·∫£ v·ªÅ StreamingHttpResponse:
```python
# Tr·∫£ response SSE ƒë·ªÉ client (nh∆∞ brower) nh·∫≠n t·ª´ng event
...
resp = StreamingHttpResponse(event_stream(), content_type="text/event-stream")
# gi√∫p proxy/nginx kh√¥ng buffer SSE
resp["Cache-Control"] = "no-cache"
resp["X-Accel-Buffering"] = "no"
return resp
```
* S·ª≠a ƒë·ªïi t∆∞∆°ng ·ª©ng cho file rag_graph.py ƒë·ªÉ h·ªó tr·ª£ streaming:
```python
class QAState(TypedDict, total=False):
    ...
    emit: Callable | None

    
# H√†m ph·ª• tr·ª£ ƒë·ªÉ ph√°t s·ª± ki·ªán
def _emit(state: QAState, event: str, data):
    emit = state.get("emit", None)
    if callable(emit):
        print(f"[EMIT] event={event} data={data}")
        emit(event, data)

        
def retrieve_node(state: QAState) -> QAState:
    _emit(state, "phase", "retrieve_start")
    ...

    for d in docs:
        src = d.metadata.get("source", "unknown")
        snippet = (d.page_content or "").strip().replace("\n", " ")[:200]
        _emit(state, "source", {"source": src, "snippet": snippet})

    _emit(state, "phase", "retrieve_done")
    return {"context": docs}


def grade_node(state: QAState) -> QAState:
    _emit(state, "phase", "grade_start")
    ...
    
    for d in docs:
        ...
        if res.startswith("Y"):
            kept.append(d)
            _emit(state, "grade", {"source": d.metadata.get("source", "Unknow"), "decision": "keep"})
        else:
            _emit(state, "grade", {"source": d.metadata.get("source", "Unknow"), "decision": "drop"})

        if not kept:
            kept = docs[:3]
    _emit(state, "phase", "grade_done")
    return {"context": kept}


def answer_node(state: QAState) -> QAState:
    _emit(state, "phase", "answer_start")
    ...

    answer_chain = _ANSWER_PROMPT | _ANSWER_LLM  #| (lambda x: x.content.strip()) # B·ªè lambda ƒë·ªÉ h·ªó tr·ª£ streaming
    
    buffer = []
    for chunk in answer_chain.stream({
        "question": q,
        "context": context
    }):
        piece = getattr(chunk, "content", "")
        if piece:
            buffer.append(piece)
            _emit(state, "token", piece)

    res = "".join(buffer).strip()
    _emit(state, "phase", "answer_done")
    return {"answer": res}


def judge_node(state: QAState) -> QAState:
    _emit(state, "phase", "judge_start")
    ...
    
    _emit(state, "verdict", verdict)
    return {"verdict": verdict}


def rewrite_node(state: QAState) -> QAState:
    _emit(state, "phase", "rewrite_start")
    ...

    _emit(state, "rewrite", {"new_query": res, "iter": iters})
    return {"rewritten": res, "iterations": iters}
```
* Th√™m class custom SerializerProtocol (m·∫∑c ƒë·ªãnh ·ªü MemorySaver() l√† None) ƒë·ªÉ x·ª≠ l√≠ l·ªói khi truy·ªÅn Callable v√†o state:
```python
class CustomSerdeProtocol(JsonPlusSerializer):
    def dumps(self, obj):
        # L·ªçc b·ªè c√°c h√†m tr∆∞·ªõc khi serialize, v√¨ ch√∫ng ta c≈©ng ch·ªâ d√πng m·ªói dict n√™n :))
        if isinstance(obj, dict):
            filtered_obj = {k: v for k, v in obj.items() if not callable(v)}
            return json.dumps(filtered_obj, default=self._default, ensure_ascii=False).encode(
                "utf-8", "ignore"
            )
        return json.dumps(obj, default=self._default, ensure_ascii=False).encode(
            "utf-8", "ignore"
        )
```
* Sau ƒë√≥ ch·ªânh l·∫°i t∆∞∆°ng ·ª©ng ·ªü h√†m build_graph():
```python
memory = MemorySaver(serde=CustomSerdeProtocol())
```
* Ch·ªânh s·ª≠a file copilot/urls.py ƒë·ªÉ th√™m ƒë∆∞·ªùng d·∫´n m·ªõi:
```python
urlpatterns = [
    path("api/ask", api_ask, name="api_ask"),
    path("api/ask/stream", api_ask_stream, name="api_ask_stream"),
    path("ask", page_ask, name="page_ask"),
]
```
* Ch·ªânh s·ª≠a template ask.html t·∫°o n√∫t "H·ªèi (stream)" ƒë·ªÉ g·ªçi /api/ask_stream:
```html
...
<button id="btn">H·ªèi (non-stream)</button>
<button id="btns">H·ªèi (stream)</button>
...

<script>
  ...
  document.getElementById('btns').onclick = async () => {
    const q = document.getElementById('q').value;
    const res = await fetch('/api/ask/stream', {
      method: 'POST',
      headers: {'Content-Type': 'text/event-stream'},
      body: JSON.stringify({question: q, k: 6, thread_id: thread})
    });
    const reader = res.body.getReader();
    const dec = new TextDecoder();
    let buf = '';
    while (true) {
      const {value, done} = await reader.read();
      if (done) break;
      buf += dec.decode(value, {stream: true});
      // simple SSE parsing
      const parts = buf.split('\n\n');
      buf = parts.pop();
      for (const evt of parts) {
        const lines = evt.split('\n');
        let type = 'message', data = '';
        for (const ln of lines) {
          if (ln.startsWith('event:')) type = ln.slice(6).trim();
          if (ln.startsWith('data:'))  data += ln.slice(5);
        }
        if (type === 'token') {
          out.textContent += data; // append token
        } else if (type === 'source') {
          const s = JSON.parse(data);
          out.textContent += `\n[SRC] ${s.source}: ${s.snippet}\n`;
        } else if (type === 'grade') {
          const s = JSON.parse(data);
          out.textContent += `\n[SRC] ${s.source}: ${s.decision}\n`;
        } else if (type === 'verdict') {
          out.textContent += `\nVerdict: ${data}\n`;
        } else if (type === 'phase') {
          out.textContent += `\n>> ${data.toUpperCase()} <<\n`;
        } else if (type === 'final') {
          const j = JSON.parse(data);
          thread = j.thread_id || thread;
          out.textContent += `\n\n=== VERDICT: ${j.verdict.toUpperCase()} ===\n`;
        } else if (type === 'rewrite') {
          const info = JSON.parse(data);
          out.textContent += `\n[REWRITE] ‚Üí ${info.new_query} (iter ${info.iter})\n`;
        } else if (type === 'error') {
          const err = JSON.parse(data);
          out.textContent += `\n[ERROR] ${err.type}: ${err.msg}\n`;
        }
      }
    }
  };
</script>
...
```

* Test th·ª≠: truy c·∫≠p http://localhost:8000/ask, nh·∫≠p c√¢u h·ªèi "M·ªánh Kim h·ª£p m√†u g√¨?" r·ªìi b·∫•m "H·ªèi (stream)".
Ch√∫ √Ω ph·∫ßn c√¢u tr·∫£ l·ªùi s·∫Ω th·∫•y n√≥ hi·ªán d·∫ßn l√™n nh∆∞ trong ChatGPT, GROK,... v·∫≠y.
* Ch√∫ng ta c√≥ th·ªÉ so s√°nh 2 b·∫£n non-stream v√† stream trong 2 h√¨nh d∆∞·ªõi ƒë√¢y:
![demo_template_screen_non_stream.jpeg](images/demo_template_screen_non_stream.jpeg)
![demo_template_screen.jpeg](images/demo_template_screen_stream.jpeg)

# B√†i 8: Reranker ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c ng·ªØ c·∫£nh

